{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_voMeDGwbAWa"
      },
      "outputs": [],
      "source": [
        "#Purpose: Do change point fitting to time series data with fixed time-binwidths and gaussian noise\n",
        "#Adapted From: Li and Yang, J Phys Chem B, 123, 689-701 (2019) https://pubs.acs.org/doi/full/10.1021/acs.jpcb.8b10561\n",
        "#Written By: Jessica Kline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsFvt7IdGJXV",
        "outputId": "1daf5951-b53a-4721-c9bb-dd1c5d7d2c47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Obuovl2qaLZX",
        "outputId": "17e43883-4ffd-434e-c161-6cd812f85901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bottleneck\n",
            "  Downloading Bottleneck-1.3.6-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.2/355.2 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from bottleneck) (1.21.6)\n",
            "Installing collected packages: bottleneck\n",
            "Successfully installed bottleneck-1.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting line_profiler\n",
            "  Downloading line_profiler-4.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (673 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m673.6/673.6 KB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: line_profiler\n",
            "Successfully installed line_profiler-4.0.2\n"
          ]
        }
      ],
      "source": [
        "#@title Imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from random import seed\n",
        "from random import random\n",
        "from random import shuffle\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import rankdata\n",
        "from scipy.optimize import curve_fit\n",
        "from scipy.special import gammainc\n",
        "from scipy.special import gamma\n",
        "import copy\n",
        "import bisect\n",
        "import math\n",
        "import mpmath\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from numba import jit\n",
        "\n",
        "!pip install bottleneck\n",
        "import bottleneck as bn\n",
        "\n",
        "! pip install line_profiler\n",
        "%load_ext line_profiler\n",
        "\n",
        "from scipy.signal import argrelextrema\n",
        "\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9ah6lWI27DrT"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['font.size'] = 22\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "plt.rcParams['font.sans-serif'] = 'Arial'\n",
        "plt.rcParams['xtick.major.pad']='10'\n",
        "plt.rcParams['ytick.major.pad']='4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "h3rLHLy0WXEt"
      },
      "outputs": [],
      "source": [
        "my_colors = ['dimgrey', [242/255, 133/255, 125/255], [254/255, 185/255, 95/255], [126/255, 188/255, 118/255],'tab:blue']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "_z90XO1NaY6Q"
      },
      "outputs": [],
      "source": [
        "#@title create_data(int delta_I)\n",
        "# this code creates synthetic data of a variable number of intensity levels following a 1.5 powerlaw\n",
        "# the intensity levels range from max_I to (max_I - n*delta_I > min_I)\n",
        "def create_data(max_I, min_I, delta_I, delta_t):\n",
        "  #generate list of delta-time changepoints assuming power law exponent of 1.5\n",
        "  time = [float(x*delta_t) for x in range(1,2001)]\n",
        "  P_t = [t**-1.5 for t in time]\n",
        "  P_selec = []\n",
        "  t_selec = []\n",
        "  for n in range (80):\n",
        "    P_selec.append(len(time)*random())\n",
        "  P_selec = np.sort(P_selec)\n",
        "  t_selec = P_selec\n",
        "  change_times = [time[int(t_selec[n])] for n in range(len(t_selec))]\n",
        "\n",
        "  #create a list of the number of events associated with each change point assuming a power law with exponent of 1.5\n",
        "  events = []\n",
        "  for n in range(len(P_selec)):\n",
        "    if n == 0:\n",
        "      a = 0\n",
        "    else:\n",
        "      a = time[int(t_selec[n-1])]\n",
        "    if n == len(P_selec)-1:\n",
        "      b = time[-1]\n",
        "    else:\n",
        "      b = time[int(t_selec[n+1])]\n",
        "    events.append(time[int(t_selec[n])]**-1.5 *(a+b)/2)\n",
        "\n",
        "  #generate a list of randomly ordered real-time changpoints\n",
        "  events = events/np.amin(events)\n",
        "  total_time = np.sum(np.multiply(events, change_times))\n",
        "  f = time[-1]\n",
        "  time_list = []\n",
        "  for n in range(len(events)):\n",
        "    cnt = 0\n",
        "    while cnt < events[n]:\n",
        "      time_list.append(change_times[n])\n",
        "      cnt = cnt + 1\n",
        "  shuffle(time_list)\n",
        "\n",
        "  event_dist_num = random()\n",
        "  \n",
        "  #create list of intensities based on the changepoints\n",
        "  I = []\n",
        "  act_cp = []\n",
        "  current_I = max_I             #change this number to change the maximum simulated intensity\n",
        "  x_t = 0\n",
        "  mult = 1\n",
        "  for n in range(len(time_list)):\n",
        "    cnt = 0\n",
        "    while cnt < time_list[n]:\n",
        "      I.append(np.random.normal(current_I, 0.03*current_I))\n",
        "      cnt = cnt + delta_t\n",
        "      x_t = x_t + delta_t\n",
        "    if current_I == max_I or current_I == min_I:\n",
        "      mult = 1\n",
        "    else:\n",
        "      mult = 3\n",
        "    if mult*random() >= event_dist_num:\n",
        "      current_I = current_I-delta_I\n",
        "    if current_I < min_I:     #change this number to change the minimum simulated intensity\n",
        "      current_I = max_I       #change this number to change the maximum simulated intensity\n",
        "    if (x_t-delta_t < 400):\n",
        "      act_cp.append(x_t-delta_t)\n",
        "\n",
        "  #trim intensity and time lists to about 7 mins\n",
        "  time = [float(x*delta_t) for x in range(0,8000)]\n",
        "  I = I[0:8000]\n",
        "  time = time[0:8000]\n",
        "\n",
        "\n",
        "  data = I\n",
        "  data = np.array(data)\n",
        "  return data, time, act_cp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "zCStAcEP1GAP"
      },
      "outputs": [],
      "source": [
        "#@title helper functions\n",
        "\n",
        "def linear_power_law(t, C, alpha):\n",
        "  #returns a linearized power law y = Ct^(-alpha)\n",
        "  return -alpha*np.log(t) + np.log(C)\n",
        "\n",
        "def linear_trunc_power_law(t, C, alpha, T_c):\n",
        "  #returns a linearized power law with truncation factor y = Ct^(-alpha)e^(t/Tc)\n",
        "  return -alpha*np.log(t) + np.log(C) + t/T_c\n",
        "\n",
        "def R_sq(fit, ydata):\n",
        "  #cacluates the r^2 of a fit\n",
        "  residual = ydata-fit\n",
        "  ss_resid = np.sum(residual**2)\n",
        "  ss_tot = np.sum((ydata-np.mean(ydata))**2)\n",
        "  return 1- (ss_resid/ss_tot)\n",
        "\n",
        "@jit\n",
        "def gauss(x, I, sigma):\n",
        "  #returns the value of a gaussian with central mean I and std_dev sigma for value x\n",
        "  return np.exp(-1*(((x-I)/sigma)**2)/2)/(sigma*math.sqrt(2*math.pi))\n",
        "\n",
        "def CriVal(N,alpha):\n",
        "  # calculate the critical value (cv) given the number of data point(N) and type-I\n",
        "  # error rate. \n",
        "\n",
        "  yd=-math.log(-1/2*math.log(alpha));\n",
        "  x=math.log(N);\n",
        "  a=math.sqrt(2*math.log(x));\n",
        "  b=2*math.log(x)+math.log(math.log(x));\n",
        "  cv=(yd+b)/a;\n",
        "\n",
        "  return cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "rAuJTJfZClmM"
      },
      "outputs": [],
      "source": [
        "#@title ndarray cp = findcp(ndarray data)\n",
        "#find the change points in the data\n",
        "\n",
        "def findcp(data):\n",
        "  #empty array to contain all the data\n",
        "  traj = [[]]*1\n",
        "\n",
        "  #first array box is the entire set of data\n",
        "  traj[0] = list(data)\n",
        "  \n",
        "  n = 0\n",
        "  while n < (len(traj)):    #iterate over the length of the entire segmented trace\n",
        "    f = [[1]]*2\n",
        "    while len(traj[n]) > 4 and bool(list(f[0])):    #iterate over each segment to find all change points\n",
        "      L = np.zeros([len(data),1])\n",
        "      cri_val = np.zeros([len(data),1])      \n",
        "      sigma_0 = bn.nanstd(traj[n])\n",
        "\n",
        "      # caculate the Log_likelihood of a change point (L) and the critical value (95% confidence) which it must be above for a change point to have occured\n",
        "      for k in range(3, len(traj[n])-2):\n",
        "        sigma_1 = bn.nanstd(traj[n][0:k+1])\n",
        "        sigma_2 = bn.nanstd(traj[n][k+1:len(traj[n])])\n",
        "\n",
        "        if sigma_2 == 0:\n",
        "          sigma_2 = 1e-12\n",
        "        if sigma_1 == 0:\n",
        "          sigma_1 = 1e-12\n",
        "        if sigma_0 == 0:\n",
        "          sigma_0 = 1e-12\n",
        "        L[k] = -(k)/2*math.log(sigma_1**2) - (len(traj[n])-k)/2*math.log(sigma_2**2) + (len(traj[n]))/2*math.log(sigma_0**2)\n",
        "        cri_val[k] = CriVal(k, 1-0.2)\n",
        "      ## end for\n",
        "\n",
        "      #find the point R greater than the critical where it is most likely for a change point to have occured\n",
        "      Z = np.sqrt(2*L)\n",
        "      z_greater = np.greater(Z, cri_val)\n",
        "      Z = np.multiply(Z, z_greater) \n",
        "      R = np.argmax(Z)\n",
        "      \n",
        "      #split data around change point R and adjust trajectory array accordingly\n",
        "      f = np.split(traj[n],[R])\n",
        "      if len(f[0]) > 0:\n",
        "        traj[n] = f[0]\n",
        "        traj.insert(n+1,f[1])\n",
        "    ## end while 2\n",
        "    n = n+1\n",
        "  ## end while 1\n",
        "\n",
        "  #convert delta_points into the real_point locations of the change points\n",
        "  prev_cp = 0;\n",
        "  all_cp = []\n",
        "  for n in range(len(traj)):\n",
        "    all_cp.append(len(traj[n])+prev_cp)\n",
        "    prev_cp = prev_cp + len(traj[n])\n",
        "\n",
        "  all_cp = np.unique(all_cp)\n",
        "\n",
        "  return all_cp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G_FcfR2dqr5K",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title dict Yi = AHclusterN(ndarray traj,  ndarray cp)\n",
        "#This section does agglomerate hierarcy clustering. Essentially what is does is\n",
        "#   using the changepoints calculated previously determine the median intensity and the std_dev of each section\n",
        "#   and then do a log-likelyhood test to see which two sections should be combined until the data is all assigned to one intensity level\n",
        "#   all n of these groupings are returned\n",
        "\n",
        "def concat(tr_1, tr_2):\n",
        "  tr_concat = np.zeros([len(tr_1)+len(tr_2)])\n",
        "  tr_concat[0:len(tr_1)] = tr_1\n",
        "  tr_concat[len(tr_1):len(tr_1)+len(tr_2)] = tr_2\n",
        "  return tr_concat\n",
        "\n",
        "# calculate the log-likelyhood\n",
        "def MN_Calc(section_1, section_2):\n",
        "  temp = concat(section_1['tr'], section_2['tr'])\n",
        "  comb_sigma = bn.nanstd(temp)\n",
        "  if comb_sigma == 0:\n",
        "    comb_sigma = 1e-12\n",
        "  comb_I = bn.median(temp)\n",
        "  return math.log(section_1['std']*section_2['std']/(comb_sigma**2)) - ((section_1['intensity'] - comb_I)**2 + (section_2['intensity'] - comb_I)**2)/(2*(comb_sigma**2))\n",
        "\n",
        "#turn array of different sized lists into a square matrix with empty cells filled by NaN\n",
        "def boolean_indexing(v):\n",
        "    lens = np.array([len(item) for item in v])\n",
        "    mask = lens[:,None] > np.arange(lens.max()+1)\n",
        "    mask = np.fliplr(mask)\n",
        "    out = np.zeros(mask.shape,dtype=float)\n",
        "    out[:] = np.NaN\n",
        "    out[mask] = np.concatenate(v)\n",
        "    return out\n",
        "\n",
        "#AH clustering process\n",
        "def AHclusterN(traj,cp):\n",
        "  cp.append(len(traj))\n",
        "  Ng_max = len(cp)\n",
        "  Yi = [{'tr':0, 'intensity':0, 't':0, 'group':[0 for x in range(Ng_max)], 'std':0} for i in range(Ng_max)]\n",
        "\n",
        "  # determine inital len(cp) sections intensities and std_devs\n",
        "  for i in range(Ng_max):\n",
        "    if i == 0:\n",
        "      start_ = 0\n",
        "      end_ = cp[i]+1\n",
        "    else:\n",
        "      start_ = cp[i-1]+1\n",
        "      end_ = cp[i]+1\n",
        "    Yi[i]['tr'] = traj[start_:end_]\n",
        "    Yi[i]['intensity'] = bn.median(Yi[i]['tr'])\n",
        "    Yi[i]['std'] = bn.nanstd(Yi[i]['tr'])\n",
        "    if Yi[i]['std'] == 0:\n",
        "      Yi[i]['std'] = 1E-50\n",
        "    Yi[i]['group'][-1] = i\n",
        "    Yi[i]['t'] = len(Yi[i]['tr'])\n",
        "\n",
        "  Yi_comb = copy.deepcopy(Yi)\n",
        "\n",
        "  M_mn = [[MN_Calc(Yi_comb[m], Yi_comb[n]) for m in range(n+1, len(Yi_comb))] for n in range(len(Yi_comb))]\n",
        "  M_mn = boolean_indexing(M_mn).T\n",
        "\n",
        "\n",
        "  # combine states to have len(cp)-1:1 states\n",
        "  for i in range(len(Yi) - 1, 0, -1):\n",
        "    mask = np.ones(M_mn.shape, dtype=bool)\n",
        "    loc_mn= np.unravel_index(bn.nanargmax(M_mn, axis=None), M_mn.shape)\n",
        "    mn_max = np.amax(loc_mn)\n",
        "    mn_min = np.amin(loc_mn)\n",
        "\n",
        "    group_min = np.amin([Yi_comb[mn_max]['group'][i],Yi_comb[mn_min]['group'][i]])\n",
        "    group_max = np.amax([Yi_comb[mn_max]['group'][i],Yi_comb[mn_min]['group'][i]])\n",
        "\n",
        "    # change state assignments for the selected states to combine\n",
        "    for nn in range(len(Yi)):\n",
        "      if Yi[nn]['group'][i] == group_min or Yi[nn]['group'][i] == group_max:\n",
        "        Yi[nn]['group'][i-1] = group_min\n",
        "      else:\n",
        "        Yi[nn]['group'][i-1] = Yi[nn]['group'][i]\n",
        "\n",
        "    for nn in range(len(Yi_comb)):\n",
        "      Yi_comb[nn]['group'][i-1] = Yi_comb[nn]['group'][i]\n",
        "\n",
        "    # combine the two states\n",
        "    Yi_comb[mn_min]['tr'] = np.concatenate((Yi_comb[mn_max]['tr'], Yi_comb[mn_min]['tr']))\n",
        "    Yi_comb[mn_min]['intensity'] = bn.median(Yi_comb[mn_min]['tr'])\n",
        "    Yi_comb[mn_min]['std'] = bn.nanstd(Yi_comb[mn_min]['tr'])\n",
        "    if Yi_comb[mn_min]['std'] == 0:\n",
        "      Yi_comb[mn_min]['std'] = 1E-50\n",
        "    Yi_comb[mn_min]['group'][i-1] = group_min\n",
        "    Yi_comb.pop(mn_max)\n",
        "\n",
        "\n",
        "    #delete set of rows/columns associated with segment that got comibined\n",
        "    mask[:][mn_max] = False\n",
        "    mask = mask.T\n",
        "    mask[:][mn_max] = False\n",
        "    M_mn = np.reshape(M_mn[mask], [len(M_mn)-1, len(M_mn)-1])\n",
        "\n",
        "    #recalculate values in M_mn affected by the combining of the two segments\n",
        "    M_mn[mn_min][:] = [MN_Calc(Yi_comb[mn_min], Yi_comb[n]) if mn_min > n else math.nan for n in range(len(M_mn[0]))]\n",
        "\n",
        "    M_mn[:][mn_min] = [MN_Calc(Yi_comb[m], Yi_comb[mn_min])  if mn_min < m else math.nan for m in range(len(M_mn[0]))]\n",
        "\n",
        "\n",
        "        \n",
        "  # change the state assignment numbers such that they range [0, n] w/ step = 1\n",
        "  for n in range(len(Yi[0]['group'])):\n",
        "    group_lst = [Yi[m]['group'][n] for m in range(len(Yi))]\n",
        "    group_rank = rankdata(group_lst, method = 'dense')\n",
        "    for m in range(len(Yi)):\n",
        "      Yi[m]['group'][n] = group_rank[m]-1\n",
        "  \n",
        "  return Yi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "8ALlipTPxvpJ"
      },
      "outputs": [],
      "source": [
        "#@title dict Yem = EMclusterN(dict Yi)\n",
        "# This takes the data from AHclusterN and decides which intensity levels are actually the same\n",
        "#   it runs on every returned grouping from AHclusterN and can only decrease the number of intensity levels\n",
        "\n",
        "def EMclusterN(Yi):\n",
        "  Yem = [[{'state':0, 'intensity':0, 'sigma':0, 'prob':0, 'pk':0, 'nos':0} for i in range(len(Yi))] for j in range(len(Yi))]\n",
        "  array_of_states = np.zeros([len(Yi), len(Yi)])\n",
        "  array_of_intensities = np.zeros([len(Yi), len(Yi)])\n",
        "  array_of_lengths = np.zeros([len(Yi), len(Yi)])\n",
        "  array_of_stddevs_sq = np.zeros([len(Yi), len(Yi)])\n",
        "\n",
        "\n",
        "  # create stock arrays of the various intensities, lengths and std_devs for all the segments\n",
        "  for r in range(len(Yi)):\n",
        "    for j in range(len(Yi)):\n",
        "      array_of_states[j, r] = Yi[r]['group'][j]\n",
        "      array_of_intensities[j,r] = Yi[r]['intensity']\n",
        "      array_of_lengths[j,r] = Yi[r]['t']\n",
        "      array_of_stddevs_sq[j,r] = Yi[r]['std']**2\n",
        "\n",
        "\n",
        "\n",
        "  for total_clusters in range(len(Yi)):\n",
        "\n",
        "    #for runtime purposes complete maximization is only run on the AH results from 10 possible states on down\n",
        "    if total_clusters < 10:\n",
        "\n",
        "      #STEP 1\n",
        "      #p_mj is the array which represents the probability that state m belongs to state j\n",
        "      #it's initialized with the AH results\n",
        "\n",
        "      num_states = np.unique(np.transpose(array_of_states[total_clusters][:]))\n",
        "      j_length = len(Yi[0]['group'])\n",
        "      m_length = int(np.amax(num_states)+1)\n",
        "      delta_p_mj = 1\n",
        "\n",
        "      intensity_arr = array_of_intensities[:][0:m_length]\n",
        "      len_arr = array_of_lengths[:][0:m_length]\n",
        "      stddev_arr = array_of_stddevs_sq[:][0:m_length]\n",
        "\n",
        "      p_mj = np.zeros([len(Yi[0]['group']), int(np.amax(num_states)+1)])\n",
        "      for n in range(len(Yi[0]['group'])):\n",
        "        p_mj[n][Yi[n]['group'][total_clusters]] = 1\n",
        "      p_mj = np.transpose(p_mj)\n",
        "\n",
        "      iter = 0\n",
        "      #this while loop runs until there is a sufficently small change in p_mj or the maximum number of iterations is exceeded\n",
        "      while delta_p_mj > 1e-8 and iter < 1e4:\n",
        "\n",
        "        #STEP 2\n",
        "        old_p_mj = np.copy(p_mj)\n",
        "\n",
        "        #calculate the intensity (I_m), std_dev (sigma_m) and probabilties (P_m) of every state\n",
        "        p_mj_x_N_j = np.multiply(p_mj, len_arr)\n",
        "        I_m = bn.nansum(np.multiply(intensity_arr, p_mj_x_N_j), axis = 1)/bn.nansum(p_mj_x_N_j, axis = 1)\n",
        "        sigma_m = np.sqrt(np.sum(np.multiply(stddev_arr, p_mj_x_N_j), axis = 1)/bn.nansum(p_mj_x_N_j, axis = 1))\n",
        "        P_m = bn.nansum(p_mj_x_N_j, axis = 1)/bn.nansum(len_arr, axis = 1)\n",
        "\n",
        "        #STEP 3\n",
        "        #caculate the new p_mj from I_m, sigma_m and P_m\n",
        "        p_mj = [np.multiply(P_m, gauss(intensity_arr[0][j], I_m,sigma_m)) for j in range(j_length)]\n",
        "        bottom_sum = np.reshape(np.repeat(np.reshape(np.sum(p_mj, axis = 1), [1,-1])[0],total_clusters+1), [-1,total_clusters+1])\n",
        "        p_mj = np.transpose(np.divide(p_mj,bottom_sum))\n",
        "\n",
        "        #calculate the difference\n",
        "        delta_p_mj = np.sum(abs(np.subtract(p_mj, old_p_mj)))\n",
        "        iter = iter +1\n",
        "      ###end while\n",
        "\n",
        "      # truncate I_m and std_m to check if the assigned states are the same to the 1's place\n",
        "      I_m_trunc = [int(a) for a in I_m if a!=0 and not math.isnan(a)]\n",
        "      std_trunc = np.array([int(a * 10**2)/10**2 for a in sigma_m if a!=0 and not math.isnan(a)])\n",
        "\n",
        "      I_m_lst = []\n",
        "\n",
        "      #assign the parameters of each state based on the condesned values and the maximum occupation probability in p_mj\n",
        "      for section in range(j_length):\n",
        "        arg_max = np.argmax(p_mj[:,section])\n",
        "        I_m_lst.append(int(I_m[arg_max]))\n",
        "        Yem[total_clusters][section]['state'] = arg_max\n",
        "        Yem[total_clusters][section]['intensity'] = I_m[arg_max]\n",
        "        Yem[total_clusters][section]['sigma'] = sigma_m[arg_max]\n",
        "        Yem[total_clusters][section]['prob'] = np.sum(p_mj[:, arg_max])\n",
        "        Yem[total_clusters][section]['pk'] = P_m[arg_max]\n",
        "        Yem[total_clusters][section]['nos'] = len(np.unique(I_m_trunc))\n",
        "\n",
        "        #Sometimes the EMOpt will say there are two states but that all segments are in the same state\n",
        "        #   this if statement addresses that unique problem\n",
        "        if section == j_length -1 and len(np.unique(I_m_lst)) != len(np.unique(I_m_trunc)):\n",
        "          I_u, I_idx = np.unique(I_m_trunc, return_index=True)\n",
        "          I_u = I_u[np.argsort(I_idx)]\n",
        "          std_u = std_trunc[np.argsort(I_idx)]\n",
        "          force_fitting(Yem, Yi, j_length, total_clusters, array_of_intensities[:][0:len(I_u)], array_of_lengths[:][0:len(I_u)], array_of_stddevs_sq[:][0:len(I_u)], I_u, std_u)\n",
        "    else:\n",
        "      # assign the AH data as the solution for these data\n",
        "      for section in range(j_length):\n",
        "        Yem[total_clusters][section]['state'] = Yi[total_clusters]['group'][section]\n",
        "        Yem[total_clusters][section]['intensity'] = Yi[total_clusters]['intensity']\n",
        "        Yem[total_clusters][section]['sigma'] = Yi[total_clusters]['std']\n",
        "        Yem[total_clusters][section]['prob'] = 1e-12\n",
        "        Yem[total_clusters][section]['pk'] = 1e-12\n",
        "        Yem[total_clusters][section]['nos'] = total_clusters\n",
        "\n",
        "  return Yem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "id": "jQZRJUtn2tSf"
      },
      "outputs": [],
      "source": [
        "#@title void force_fitting(dict Yem, dict Yi, int j_length, int total_clusters, ndarray intensity_arr, ndarray len_arr, ndarray stddev_arr, list I_u, list std_u)\n",
        "#Find the fitting of the EMOpt based on the number of states it returned by reinitializing p_mj and performing\n",
        "#   one more iteration of the calculating loop\n",
        "\n",
        "def force_fitting(Yem, Yi, j_length, total_clusters, intensity_arr, len_arr, stddev_arr, I_u, std_u):\n",
        "  p_mj = np.zeros([len(Yi[0]['group']), len(I_u)])\n",
        "  for n in range(len(Yi[0]['group'])):\n",
        "    pos_gauss = []\n",
        "    for m in range(len(I_u)):\n",
        "      pos_gauss.append(gauss(Yi[n]['intensity'], I_u[m], std_u[m]))\n",
        "    max_loc = np.argmax(pos_gauss)\n",
        "    p_mj[n][max_loc] = 1\n",
        "  p_mj = np.transpose(p_mj)\n",
        "\n",
        "  p_mj_x_N_j = np.multiply(p_mj, len_arr)\n",
        "  I_m = np.nansum(np.multiply(intensity_arr, p_mj_x_N_j), axis = 1)/np.nansum(p_mj_x_N_j, axis = 1)\n",
        "  sigma_m = np.sqrt(np.sum(np.multiply(stddev_arr, p_mj_x_N_j), axis = 1)/np.nansum(p_mj_x_N_j, axis = 1))\n",
        "  P_m = np.nansum(p_mj_x_N_j, axis = 1)/np.nansum(len_arr, axis = 1)\n",
        "\n",
        "  p_mj = [[P_m[m]*p_mj[m][j] for m in range(len(I_u))] for j in range(j_length)]\n",
        "  p_mj = np.transpose(p_mj)\n",
        "\n",
        "  for section in range(j_length):\n",
        "    arg_max = np.argmax(p_mj[:,section])\n",
        "    Yem[total_clusters][section]['state'] = arg_max\n",
        "    Yem[total_clusters][section]['intensity'] = I_m[arg_max]\n",
        "    Yem[total_clusters][section]['sigma'] = sigma_m[arg_max]\n",
        "    Yem[total_clusters][section]['prob'] = np.sum(p_mj[:, arg_max])\n",
        "    Yem[total_clusters][section]['pk'] = P_m[arg_max]\n",
        "    Yem[total_clusters][section]['nos'] = len(np.unique(I_m))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "id": "AY7dKkjnxBEf"
      },
      "outputs": [],
      "source": [
        "#@title ndaray state_para = get_state(int Ns, int k, ndarray Yem)\n",
        "\n",
        "def get_state(Ns, k, Yem):\n",
        "\n",
        "  # Extract parameters of states.\n",
        "\n",
        "  # state_para: \n",
        "  # 1st column: intensity levels of states.\n",
        "  # 2nd column: noise levels(standard deviation) of states.\n",
        "  # 3rd column: populations of states.\n",
        "  # the number of rows in state_para corresponds to the number of states,\n",
        "  # each row include the parameters from one state.\n",
        "  # other parameters are added later\n",
        "  # 4th column: <t> for each state\n",
        "  # 5th column: time of first dark event\n",
        "  \n",
        "  state_para = np.zeros([Ns,6])\n",
        "  g = 0\n",
        "  s = 0\n",
        "  while s < Ns:\n",
        "      if ~np.isin(Yem[k][g]['intensity'],state_para[:,0]):\n",
        "          state_para[s,0] = Yem[k][g]['intensity']\n",
        "          state_para[s,1] = Yem[k][g]['sigma']\n",
        "          state_para[s,2] = Yem[k][g]['pk']\n",
        "          s = s + 1\n",
        "      g = g + 1\n",
        "      if g >= len(Yem):\n",
        "          break\n",
        "  \n",
        "  return state_para"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "id": "Whs9HV5cRPf8"
      },
      "outputs": [],
      "source": [
        "#@title ndarray state_trace = plotting_prework(int k, ndarray state_para, dict Yem, dict Yi, ndarray data)\n",
        "# this guy just gets some stuff ready for the plotting we want to do later\n",
        "\n",
        "def between(num, lower, upper):\n",
        "  return (num >= lower and num <= upper)\n",
        "\n",
        "\n",
        "def plotting_prework(k, state_para, Yem, Yi, data):\n",
        "  #sorts the returned state_para by its order of occurence in the assignment\n",
        "  temp = [Yem[k][n]['state'] for n in range(len(Yem))]\n",
        "  aaa, bbb = np.unique(temp, return_index=True)\n",
        "  aaa = aaa[np.argsort(bbb)]\n",
        "\n",
        "  state_para1 = np.zeros([int(np.amax(aaa)+1),6])\n",
        "  for n in range(len(aaa)):\n",
        "    state_para1[aaa[n],:] = state_para[n,:]\n",
        "\n",
        "  state_trace = np.empty([len(data),2])\n",
        "  state_trace[:] = 0\n",
        "\n",
        "  #defines the relevant ranges to account for some fast events\n",
        "  ranges = []\n",
        "  for n in range(len(state_para1)):\n",
        "    ranges.append([state_para1[n,0]-2.5*state_para1[n,1], state_para1[n,0]+2.5*state_para1[n,1]])\n",
        "\n",
        "\n",
        "  #assign each time-point to a state and intensity level\n",
        "  #also accounts for short time changes which changepoint doesn't pick up\n",
        "  end_ = 0\n",
        "  for n in range(len(Yi)):\n",
        "    start_ = end_\n",
        "    end_ = start_+Yi[n]['t']\n",
        "    for x in range(start_, end_):\n",
        "      state_trace[x,0] = Yem[k][n]['state']\n",
        "\n",
        "      if not between(data[x], ranges[int(state_trace[x,0])][0], ranges[int(state_trace[x,0])][1]):\n",
        "        state = 0\n",
        "        while state < len(state_para[:,0]) and not between(data[x], ranges[state][0], ranges[state][1]):\n",
        "          state = state+1\n",
        "        if state != len(state_para[:,0]):\n",
        "          state_trace[x,0] = state\n",
        "      state_trace[x,1] = state_para1[int(state_trace[x,0]), 0]\n",
        "\n",
        "  #figure out time of first dark event (defined as an event not in the highest intensity state) and return  \n",
        "  f = np.argmax(state_para1[:,0])\n",
        "  loc = np.argwhere(state_trace[:,0]!= f)\n",
        "\n",
        "  if len(loc) >= 1:\n",
        "    for n in range(len(state_para1)):\n",
        "      if len(state_para1[0,:]) >1:\n",
        "        state_para1[n,5] = loc[0]*0.2\n",
        "      else:\n",
        "        state_para1[n,5] = 999999\n",
        "  else:\n",
        "    state_para1[0,5] = 999999\n",
        "\n",
        "\n",
        "  return state_trace, state_para1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellView": "form",
        "id": "EIZ0hrsXPydi"
      },
      "outputs": [],
      "source": [
        "#@title list counts_x, list prob_y, list y_fit, list params = power_law_fitting(ndarray state_trace, ndarray time, int level_of_interest)\n",
        "#fits segmented data to power law (a*t^(-m)) or truncated power law(a*t^(-m)*e^(t/Tc))\n",
        "\n",
        "def calc_avg_t(b, t_min, t_max):\n",
        "  #returns average time for power law\n",
        "  return (b+1)/(b+2) * (t_max**(b+2) - t_min**(b+2))/(t_max**(b+1) - t_min**(b+1))\n",
        "\n",
        "def calc_avg_t_trunc(b, t_c, t_min, t_max):\n",
        "  #returns average time for truncated power law\n",
        "  f = -t_c*(mpmath.gammainc(b+2, -t_max/t_c) -mpmath.gammainc(b+2, -t_min/t_c))/(mpmath.gammainc(b+1, -t_max/t_c) -mpmath.gammainc(b+1, -t_min/t_c))\n",
        "  return float(mpmath.re(f))\n",
        "\n",
        "def power_law_fitting(state_trace, time, level_of_interest):\n",
        "  t = time[1]\n",
        "  time_bin = time[1]\n",
        "  lst = []\n",
        "\n",
        "  #find all events associated with an intensity level and record their durration\n",
        "  for n in range(len(time)-1):\n",
        "    if state_trace[n,0] == level_of_interest:\n",
        "      if state_trace[n,0] == state_trace[n+1,0]:\n",
        "        t += time_bin\n",
        "      else:\n",
        "        lst.append(t)\n",
        "        t = time_bin\n",
        "  lst.append(t+time_bin)\n",
        "\n",
        "  #histogram the intenisity events\n",
        "  counts = np.histogram(lst, bins = time)\n",
        "  counts_x = [counts[1][x] for x in range(len(counts[0])) if counts[0][x] != 0]\n",
        "  counts_y = [counts[0][x] for x in range(len(counts[0])) if counts[0][x] != 0]\n",
        "  counts_x.append(time[-1])\n",
        "\n",
        "  #calcuate the probability of an event lasting a durration t\n",
        "  prob_y = []\n",
        "  for n in range(len(counts_y)):\n",
        "    if n == 0:\n",
        "      prob_y.append(counts_y[n]/((counts_x[n+1] - time_bin)/2))\n",
        "    else:\n",
        "      prob_y.append(counts_y[n]/((counts_x[n+1] - counts_x[n-1])/2))\n",
        "\n",
        "  counts_x.pop(-1)\n",
        "\n",
        "  #fit to linear power law\n",
        "  power_fit, power_cov = curve_fit(linear_power_law, counts_x, np.log(prob_y))\n",
        "  power_y = power_fit[0]*counts_x**(-power_fit[1])\n",
        "\n",
        "  #fit to truncated power law\n",
        "  trunc_power_fit, trunc_power_cov = curve_fit(linear_trunc_power_law, counts_x, np.log(prob_y))\n",
        "  trunc_power_y = trunc_power_fit[0]*counts_x**(-trunc_power_fit[1])*np.exp(counts_x/trunc_power_fit[2])\n",
        "\n",
        "  #determine R^2 of both fits\n",
        "  R_sq_power = R_sq(linear_power_law(counts_x, power_fit[0], power_fit[1]), np.log(prob_y))\n",
        "  R_sq_trunc_power = R_sq(linear_trunc_power_law(counts_x, trunc_power_fit[0], trunc_power_fit[1], trunc_power_fit[2]), np.log(prob_y))\n",
        "\n",
        "  #return the parameters of the fit\n",
        "  if R_sq_power > R_sq_trunc_power:\n",
        "    y_fit= power_fit[0]*counts_x**(-power_fit[1])\n",
        "    params = [power_fit[1], math.nan, R_sq_power, calc_avg_t(-power_fit[1], counts_x[0], counts_x[-1])]\n",
        "  else:\n",
        "    y_fit = trunc_power_fit[0]*counts_x**(-trunc_power_fit[1])*np.exp(counts_x/trunc_power_fit[2])\n",
        "    params = [trunc_power_fit[1],trunc_power_fit[2],R_sq_trunc_power, calc_avg_t_trunc(-trunc_power_fit[1], trunc_power_fit[2], counts_x[0], counts_x[-1])]\n",
        "\n",
        "\n",
        "  return counts_x, prob_y, y_fit, params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellView": "form",
        "id": "5Bb4iIHyaHZS"
      },
      "outputs": [],
      "source": [
        "#@title BIC_calc(Yi, Yem)\n",
        "#calculate the BIC for each fitting to determine the most likely fitting\n",
        "\n",
        "@jit\n",
        "def BIC_calc(Yi, Yem):\n",
        "  BIC = np.zeros([len(Yi)])\n",
        "  total_points = 0\n",
        "  for m in range(len(Yi)):\n",
        "    #for run time purpose we crop to a maximum to 10 possible states\n",
        "    if m <=10:\n",
        "      double_sum = 0\n",
        "      L_em = 0\n",
        "      G = Yem[m][0]['nos']\n",
        "      total_points = 0\n",
        "      num_segs = 1\n",
        "      last_seg = Yem[m][0]['state']\n",
        "      for j in range(len(Yi)):\n",
        "        for i in range(Yi[j]['t']):\n",
        "          if gauss(Yi[j]['tr'][i], Yem[m][j]['intensity'], Yem[m][j]['sigma']) == 0:\n",
        "            #log(0) -> goes to -inf so we add a very large negative number\n",
        "            double_sum = double_sum + math.log(Yem[m][j]['pk']) + -9999999\n",
        "          else:\n",
        "            double_sum = double_sum + math.log(Yem[m][j]['pk']) + math.log(gauss(Yi[j]['tr'][i], Yem[m][j]['intensity'], Yem[m][j]['sigma']))\n",
        "\n",
        "        if math.isnan(Yem[m][j]['prob']):\n",
        "          L_em = L_em + np.log(1e-99)\n",
        "        else:\n",
        "          L_em = L_em + np.log(Yem[m][j]['prob'])\n",
        "\n",
        "        total_points = total_points + Yi[j]['t']\n",
        "\n",
        "        if Yem[m][j]['state'] != last_seg:\n",
        "          num_segs += 1\n",
        "          last_seg = Yem[m][j]['state']\n",
        "\n",
        "      BIC[m] = double_sum + L_em - (3/2)*G*math.log(num_segs) - num_segs*math.log(total_points)/2\n",
        "    else:\n",
        "      BIC[m] = -1E22  \n",
        "  return BIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cellView": "form",
        "id": "-c6SE6veaj5w"
      },
      "outputs": [],
      "source": [
        "#@title Plotting(BIC, k,  Ns_range, state_trace, data, state_para, time)\n",
        "#all the plotting jazz for the calculations done here\n",
        "\n",
        "def Plotting(BIC, k,  Ns_range, state_trace, data, state_para, time, draw_plots):\n",
        "\n",
        "  bins_ = np.arange(np.amin(data)-10, np.amax(data)+10, 0.5)\n",
        "  h = np.histogram(data, bins=bins_)\n",
        "\n",
        "  #BIC PLOT\n",
        "  if draw_plots:\n",
        "    plt.subplot(2,4,1)\n",
        "      #plot BIC\n",
        "    plt.plot(BIC, color = my_colors[0], linewidth = 3, zorder = 0)\n",
        "      #draw circle around selected max location\n",
        "    plt.scatter(k, BIC[k], edgecolors = my_colors[-1], facecolors = 'none', zorder = 1, s = 100, linewidths = 3)\n",
        "    plt.xlabel(\"grouping number\")\n",
        "    plt.ylabel(\"BIC\")\n",
        "    plt.ylim(np.amax([BIC[k]-2000, np.amin(BIC[0:8])-500]), np.amax(BIC)+100)\n",
        "    plt.xlim([0,8])\n",
        "      #plot the final number of states per grouping on right yaxis\n",
        "    plt.gca().twinx().plot(Ns_range, color = my_colors[-1], linewidth = 3)\n",
        "    plt.ylabel(\"number of states\", color = my_colors[-1])\n",
        "    plt.ylim([0, 10])\n",
        "    plt.xticks(np.arange(0, 10, 2))\n",
        "    plt.yticks(np.arange(0, 11, 2))\n",
        "    plt.gca().spines['right'].set_color(my_colors[-1])\n",
        "    plt.gca().tick_params(axis='y', colors=my_colors[-1])\n",
        "\n",
        "    #POWER LAW PLOTS\n",
        "    if Ns_range[k] > 1:\n",
        "      for n in range(2,3+int(np.amax(state_trace[:,0]))):\n",
        "          #do power law fitting\n",
        "        counts_x, prob_y, y_fit, params = power_law_fitting(state_trace, time, n-2)\n",
        "        state_para[n-2,3] = params[0]    #alpha\n",
        "        state_para[n-2,4] = params[3]    #avg time\n",
        "        plt.subplot(2,4,n)\n",
        "          # plot fit power law\n",
        "        plt.loglog(counts_x, y_fit, color = my_colors[0], linewidth = 2)\n",
        "          # plot observed event lengths by probability\n",
        "        plt.scatter(counts_x, prob_y, color = my_colors[n-1])\n",
        "          # display parameters of the fitting\n",
        "        plt.text(np.amin(counts_x), np.amin(prob_y)+.1, \"state: \" + str(\"{:.0f}\".format(state_para[n-2,0]))+ \"\\n \\nalpha: \" + str(\"{:.2f}\".format(params[0])) +\"\\nT_c: \" + str(\"{:.2f}\".format(params[1])) + \"\\nR^2: \" + str(\"{:.2f}\".format(params[2])) + \"\\navg_t (s): \" + str(\"{:.2f}\".format(params[3])))\n",
        "        plt.xlabel('event time (s)')\n",
        "        plt.ylabel('event probability')\n",
        "\n",
        "\n",
        "    #STATE TRACE PLOT\n",
        "    plt.subplot(2,4,(5,7))\n",
        "      #plot data\n",
        "    plt.plot(time,data, color = my_colors[0], zorder = 0)\n",
        "    plt.xlim([-5,int(np.amax(time)+5)])\n",
        "    for n in range(0, int(np.amax(state_trace[:,0])+1)):\n",
        "      temp_trace = [state_trace[x,1]  if state_trace[x,0] == n else math.nan for x in range(len(state_trace[:,0]))]\n",
        "        #plot traces of each intensity state\n",
        "      plt.scatter(time, temp_trace, color = my_colors[n+1], linewidth = 3, s = 10, zorder = n+1, alpha=1)\n",
        "    plt.xlabel(\"time (s)\")\n",
        "    plt.ylabel(\"Intensity (counts/s)\")\n",
        "\n",
        "    #INTENSITY HISTOGRAM PLOT\n",
        "    plt.subplot(2,4,8)\n",
        "      #histogram the data\n",
        "    plt.hist(data, orientation='horizontal', bins= bins_, color = my_colors[0])\n",
        "    plt.ylabel(\"Intensity (counts/s)\")\n",
        "    for n in range(len(state_para[:,0])):\n",
        "      bis_bis = bisect.bisect(bins_, state_para[n,0])-1\n",
        "      while h[0][bis_bis] == 0:\n",
        "        bis_bis += 5\n",
        "      height = h[0][bis_bis]\n",
        "      gaus = norm.pdf(bins_, state_para[n,0], state_para[n,1])\n",
        "        #print the gaussians of the intensity states\n",
        "      plt.plot(gaus/np.amax(gaus)*height, bins_, linewidth = 5, color = my_colors[n+1])\n",
        "      t = plt.text(height+2,state_para[n,0], str(\"{:.0f}\".format(state_para[n,0])) + \" +/- \" + str(\"{:.0f}\".format(state_para[n,1])))\n",
        "      t.set_bbox(dict(facecolor='white', alpha=1, edgecolor='white'))\n",
        "    plt.xlabel(\"counts\")\n",
        "\n",
        "    plt.subplots_adjust(left=0.125, bottom=0.1, right=1.1, top=0.9, wspace=0.3, hspace=0.2)\n",
        "    fig = plt.gcf()\n",
        "    fig.set_size_inches(20, 10)\n",
        "  else:\n",
        "    if Ns_range[k] > 1:\n",
        "      for n in range(0,int(np.amax(state_trace[:,0]))+1):\n",
        "        #do power law fitting\n",
        "        counts_x, prob_y, y_fit, params = power_law_fitting(state_trace, time, n)\n",
        "        state_para[n,3] = params[0]    #alpha\n",
        "        state_para[n,4] = params[3]    #avg time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellView": "form",
        "id": "tX0j4JcOwl4j"
      },
      "outputs": [],
      "source": [
        "#@title CP_MAIN\n",
        "# Determine the number of states and state parameters from input trajectory.\n",
        "# data: raw trajectory (in the format of row).\n",
        "# time: time points associated with each data point in the trajectory\n",
        "# draw_plots: T/F to indicate if the data should be plotted\n",
        "\n",
        "def CP_MAIN(data, time, draw_plots):\n",
        "  #FIND THE CHANGE POINTS\n",
        "  all_cp = findcp(data)\n",
        "  all_cp = list(all_cp)\n",
        "  if all_cp[-1] == len(data):\n",
        "    all_cp.pop(-1)\n",
        "\n",
        "  #AH CLUSTERING\n",
        "  Yi = AHclusterN(data, all_cp)\n",
        "\n",
        "  #EM OPTIMIZATION\n",
        "  Yem = EMclusterN(Yi)\n",
        "\n",
        "  #make sure the states are in contiguous order\n",
        "  for n in range(len(Yem)):\n",
        "    z = [Yem[n][f]['state'] for f in range(len(Yem))]\n",
        "    z_rank = rankdata(z, method='dense')\n",
        "    for f in range(len(Yem)):\n",
        "      Yem[n][f]['state'] = z_rank[f]-1\n",
        "\n",
        "  #CALCULATE BIC\n",
        "  BIC = BIC_calc(Yi, Yem)\n",
        "\n",
        "  #find ideal grouping\n",
        "  k = np.argmax(BIC)\n",
        "\n",
        "  Ns = Yem[k][0]['nos']\n",
        "  #print(Ns)\n",
        "  Ns_range = [Yem[n][0]['nos'] for n in range(len(Yem))]\n",
        "  state_para = get_state(Ns,k,Yem)\n",
        "  np.set_printoptions(suppress=True)\n",
        "\n",
        "  state_trace, state_para = plotting_prework(k, state_para, Yem, Yi, data)\n",
        "\n",
        "  Plotting(BIC, k,  Ns_range, state_trace, data, state_para, time, draw_plots)\n",
        "\n",
        "  return state_para"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "G1sCbbuKGFlX"
      },
      "outputs": [],
      "source": [
        "#change file paths to select your data\n",
        "parent_folder = \"/content/drive/Shareddrives/Ginger Lab/IMOD/g14 data/12823_OAMOLAM_4005/analyzed_data/\"   #change to master folder\n",
        "qd_trace_folder = parent_folder + \"particle_picking/traces/\"\n",
        "qd_file_name = \"ROI1_50ms_8000im_1_MMStack_Default.csv\"                          #change to name of trace\n",
        "\n",
        "para_file_name = \"CPA Para \" + qd_file_name\n",
        "fig_file_name = qd_file_name[:-4]\n",
        "\n",
        "para_folder = parent_folder + \"CPA_params/\"\n",
        "fig_folder = para_folder + \"Example CPA Fittings/\"\n",
        "\n",
        "#read in traces\n",
        "qd_traces = pd.read_csv(qd_trace_folder+ qd_file_name)\n",
        "qd_traces = qd_traces.to_numpy()\n",
        "\n",
        "int_time_s = 0.05                 ## step_size for equidistance time bins\n",
        "\n",
        "start_trace = 1                   ## start at 1 for new batch - change for picking up in the middle of a batch\n",
        "\n",
        "auto_save = 10                    ## save the list of params every so many traces(helps with long run times)\n",
        "\n",
        "if os.path.exists(para_folder) == False:\n",
        "  os.mkdir(para_folder)\n",
        "\n",
        "if os.path.exists(fig_folder) == False:\n",
        "  os.mkdir(fig_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlDx60nhEWdh"
      },
      "outputs": [],
      "source": [
        "###### UNCOMMENT FOR SYNTHETIC DATA\n",
        "# data, time, act_cp = create_data(140, 100, 20, 0.05)          \n",
        "# para = CP_MAIN(data, time, True)\n",
        "######\n",
        "\n",
        "\n",
        "###### UNCOMMENT FOR REAL DATA\n",
        "lst_para = []\n",
        "time = [n*int_time_s for n in range(len(qd_traces[:,0]))]\n",
        "\n",
        "for n in range (start_trace, len(qd_traces[0])): #should start indexing at  one, slice zero is the bin numbers\n",
        "  try:\n",
        "    #analyze the data - every 10th trace is plotted and saved\n",
        "    if n % 10 == 0:\n",
        "      para = CP_MAIN(qd_traces[:,n], time, True)\n",
        "      plt.savefig(fig_folder+f'QD_{n}__'+fig_file_name+'.jpg', dpi = 300, bbox_inches = 'tight')\n",
        "      plt.close(plt.gcf())\n",
        "    else:\n",
        "      para = CP_MAIN(qd_traces[:,n], time, False)   #False - no plotting individual traces\n",
        "\n",
        "    #append trace CPA parameters to the list of all parameters\n",
        "    for j in range(len(para)):\n",
        "      for k in para[j,:]:\n",
        "        lst_para.append(k)\n",
        "    for f in range(6):\n",
        "      lst_para.append(math.nan)\n",
        "\n",
        "    print(f\"{n} -- ({len(para)})\")\n",
        "  except:\n",
        "    print(f\"failed {n}\")\n",
        "\n",
        "  #auto save in the middle of a batch\n",
        "  if n%auto_save == 0:\n",
        "    lst_para = np.array(lst_para).reshape([int(len(lst_para)/6), 6])\n",
        "    p_name = para_file_name[:-4] + f'({n}).csv'\n",
        "    pd.DataFrame(np.array(lst_para)).to_csv(os.path.join(para_folder,p_name))\n",
        "    lst_para = []\n",
        "\n",
        "#save at the end of a batch\n",
        "lst_para = np.array(lst_para).reshape([int(len(lst_para)/6), 6])\n",
        "p_name = para_file_name[:-4] + f'({n}).csv'\n",
        "pd.DataFrame(np.array(lst_para)).to_csv(os.path.join(para_folder,p_name))\n",
        "######"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}