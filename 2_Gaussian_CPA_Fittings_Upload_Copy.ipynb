{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**2. Gaussian CPA Fittings** \n",
        "\n",
        "Written by Jessica Kline\n",
        "\n",
        "This code is the second step in analyzing widefield blinking videos and assumes that \"1. Find QDs\" has already been run on all of the necessary data. It reads in the all of the \".csv\" files in yourdatafilepath/analyzed_data/particle_picking/traces/ and completes change point analysis. It outputs an new \".csv\" containing parameters assoicated with the CPA fit of the data.\n",
        "\n",
        "This code is designed for change point analysis on time series data with fixed time-binwdiths and a gaussian noise profile. The basis of this code was adapted from Li and Yang, J Phys Chem B, 123, 689-701 (2019) https://pubs.acs.org/doi/full/10.1021/acs.jpcb.8b10561. "
      ],
      "metadata": {
        "id": "whBSKLi5tvwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output values of the CPA fitting (ie the para array):\n",
        "*   the number of rows in state_para corresponds to the number of states\n",
        "*   each row include the parameters from one state.\n",
        "\n",
        "1st column: intensity levels of states.\n",
        "\n",
        "2nd column: noise levels(standard deviation) of states.\n",
        "\n",
        "3rd column: populations of states.\n",
        "\n",
        "4th column: expected event dwell time for each state\n",
        "\n",
        "5th column: time of first dark event"
      ],
      "metadata": {
        "id": "v50wWfvZvDgA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsFvt7IdGJXV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Obuovl2qaLZX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from random import seed\n",
        "from random import random\n",
        "from random import shuffle\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import rankdata\n",
        "from scipy.optimize import curve_fit\n",
        "from scipy.special import gammainc\n",
        "from scipy.special import gamma\n",
        "import copy\n",
        "import bisect\n",
        "import math\n",
        "import mpmath\n",
        "import pandas as pd\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "import logging\n",
        "\n",
        "from numba import jit\n",
        "from numba import njit\n",
        "\n",
        "!pip install bottleneck\n",
        "import bottleneck as bn\n",
        "\n",
        "! pip install line_profiler\n",
        "%load_ext line_profiler\n",
        "\n",
        "from scipy.signal import argrelextrema\n",
        "\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ah6lWI27DrT"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['font.size'] = 22\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "plt.rcParams['font.sans-serif'] = 'Arial'\n",
        "plt.rcParams['xtick.major.pad']='10'\n",
        "plt.rcParams['ytick.major.pad']='4'\n",
        "\n",
        "logging.getLogger('matplotlib.font_manager').disabled = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3rLHLy0WXEt"
      },
      "outputs": [],
      "source": [
        "my_colors = ['dimgrey', [242/255, 133/255, 125/255], [254/255, 185/255, 95/255], [126/255, 188/255, 118/255],'tab:blue']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title tick_settings\n",
        "def tick_settings(minor):\n",
        "  plt.gca().tick_params(bottom=True, top=True, left=True, right=True)\n",
        "  plt.tick_params(axis = 'both', direction = 'in', length = 6, width = 1.5)\n",
        "  if minor:\n",
        "    plt.gca().tick_params(which='minor', direction = \"in\", length = 4, width = 0.75)\n",
        "    plt.gca().tick_params(which='minor', bottom=True, top=True)\n",
        "\n",
        "  for axis in ['top','bottom','left','right']:\n",
        "    plt.gca().spines[axis].set_linewidth(1.5)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "P3Lw0vLga0XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_z90XO1NaY6Q"
      },
      "outputs": [],
      "source": [
        "#@title create_data\n",
        "# this code creates synthetic data of a variable number of intensity levels following a 1.5 powerlaw\n",
        "# the intensity levels range from max_I to (max_I - n*delta_I > min_I)\n",
        "\n",
        "def create_data(max_I, min_I, delta_I, delta_t):\n",
        "  #generate list of delta-time changepoints assuming power law exponent of 1.5\n",
        "  time = [float(x*delta_t) for x in range(1,2001)]\n",
        "  P_t = [t**-1.5 for t in time]\n",
        "  P_selec = []\n",
        "  t_selec = []\n",
        "  for n in range (80):\n",
        "    P_selec.append(len(time)*random())\n",
        "  P_selec = np.sort(P_selec)\n",
        "  t_selec = P_selec\n",
        "  change_times = [time[int(t_selec[n])] for n in range(len(t_selec))]\n",
        "\n",
        "  #create a list of the number of events associated with each change point assuming a power law with exponent of 1.5\n",
        "  events = []\n",
        "  for n in range(len(P_selec)):\n",
        "    if n == 0:\n",
        "      a = 0\n",
        "    else:\n",
        "      a = time[int(t_selec[n-1])]\n",
        "    if n == len(P_selec)-1:\n",
        "      b = time[-1]\n",
        "    else:\n",
        "      b = time[int(t_selec[n+1])]\n",
        "    events.append(time[int(t_selec[n])]**-1.5 *(a+b)/2)\n",
        "\n",
        "  #generate a list of randomly ordered real-time changpoints\n",
        "  events = events/np.amin(events)\n",
        "  total_time = np.sum(np.multiply(events, change_times))\n",
        "  f = time[-1]\n",
        "  time_list = []\n",
        "  for n in range(len(events)):\n",
        "    cnt = 0\n",
        "    while cnt < events[n]:\n",
        "      time_list.append(change_times[n])\n",
        "      cnt = cnt + 1\n",
        "  shuffle(time_list)\n",
        "\n",
        "  event_dist_num = random()\n",
        "  \n",
        "  #create list of intensities based on the changepoints\n",
        "  I = []\n",
        "  act_cp = []\n",
        "  current_I = max_I             \n",
        "  x_t = 0\n",
        "  mult = 1\n",
        "  for n in range(len(time_list)):\n",
        "    cnt = 0\n",
        "    while cnt < time_list[n]:\n",
        "      I.append(np.random.normal(current_I, 0.03*current_I))\n",
        "      cnt = cnt + delta_t\n",
        "      x_t = x_t + delta_t\n",
        "    if current_I == max_I or current_I == min_I:\n",
        "      mult = 1\n",
        "    else:\n",
        "      mult = 3\n",
        "    if mult*random() >= event_dist_num:\n",
        "      current_I = current_I-delta_I\n",
        "    if current_I < min_I:     \n",
        "      current_I = max_I\n",
        "    \n",
        "    act_cp.append(x_t-delta_t)\n",
        "\n",
        "  #trim intensity and time lists to about 7 mins\n",
        "  time = [float(x*delta_t) for x in range(0,8000)]\n",
        "  I = I[0:8000]\n",
        "  time = time[0:8000]\n",
        "\n",
        "\n",
        "  data = I\n",
        "  data = np.array(data)\n",
        "  return data, time, act_cp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zCStAcEP1GAP"
      },
      "outputs": [],
      "source": [
        "#@title helper functions\n",
        "\n",
        "def linear_power_law(t, C, alpha):\n",
        "  #returns a linearized power law y = Ct^(-alpha)\n",
        "  return -alpha*np.log(t) + np.log(C)\n",
        "\n",
        "def linear_trunc_power_law(t, C, alpha, T_c):\n",
        "  #returns a linearized power law with truncation factor y = Ct^(-alpha)e^(t/Tc)\n",
        "  return -alpha*np.log(t) + np.log(C) + t/T_c\n",
        "\n",
        "def R_sq(fit, ydata):\n",
        "  #cacluates the r^2 of a fit\n",
        "  residual = ydata-fit\n",
        "  ss_resid = np.sum(residual**2)\n",
        "  ss_tot = np.sum((ydata-np.mean(ydata))**2)\n",
        "  return 1- (ss_resid/ss_tot)\n",
        "\n",
        "@njit\n",
        "def gauss(x, I, sigma):\n",
        "  #returns the value of a gaussian with central mean I and std_dev sigma for value x\n",
        "  return np.exp(-1*(((x-I)/sigma)**2)/2)/(sigma*math.sqrt(2*math.pi))\n",
        "\n",
        "\n",
        "def CriVal(N,alpha):\n",
        "  # calculate the critical value (cv) given the number of data point(N) and type-I\n",
        "  # error rate. \n",
        "\n",
        "  yd=-math.log(-1/2*math.log(alpha));\n",
        "  x=math.log(N);\n",
        "  a=math.sqrt(2*math.log(x));\n",
        "  b=2*math.log(x)+math.log(math.log(x));\n",
        "  cv=(yd+b)/a;\n",
        "\n",
        "  return cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "x8hVkDEYrYtZ"
      },
      "outputs": [],
      "source": [
        "#@title NJIT find cp\n",
        "#find the change points in the data\n",
        "\n",
        "#add new value to array in NJIT\n",
        "@njit\n",
        "def np_insert(lst, index, val):\n",
        "  new_lst = [np.array([])]*(len(lst)+1)\n",
        "  new_lst[0:index] = lst[0:index]\n",
        "  new_lst[index] = val\n",
        "  new_lst[index+1:-1] = lst[index:-1]\n",
        "  \n",
        "  return new_lst\n",
        "\n",
        "#split an array in NJIT\n",
        "@njit\n",
        "def np_split(arr, val):\n",
        "  left_side = np.empty((val))\n",
        "  right_side = np.empty((len(arr)-val))\n",
        "  left_side = arr[0:val]\n",
        "  right_side = arr[val:-1]\n",
        "\n",
        "  return [left_side, right_side]\n",
        "\n",
        "\n",
        "@njit\n",
        "def NJIT_findcp(data):\n",
        "  #empty array to contain all the data\n",
        "  traj = [np.array([])]*1\n",
        "\n",
        "  #first array box is the entire set of data\n",
        "  traj[0] = data\n",
        "  \n",
        "  n = 0\n",
        "  while n < (len(traj)):    #iterate over the length of the entire segmented trace\n",
        "    f = [np.array([1])]*2\n",
        "    while len(traj[n]) > 4 and bool(list(f[0])):    #iterate over each segment to find all change points\n",
        "      L = np.zeros((len(data),1))\n",
        "      cri_val = np.zeros((len(data),1))      \n",
        "      sigma_0 = np.nanstd(traj[n])\n",
        "\n",
        "      # caculate the Log_likelihood of a change point (L) and the critical value (95% confidence) which it must be above for a change point to have occured\n",
        "      for k in range(3, len(traj[n])-2):\n",
        "        sigma_1 = np.nanstd(traj[n][0:k+1])\n",
        "        sigma_2 = np.nanstd(traj[n][k+1:len(traj[n])])\n",
        "\n",
        "        if sigma_2 == 0:\n",
        "          sigma_2 = 1e-12\n",
        "        if sigma_1 == 0:\n",
        "          sigma_1 = 1e-12\n",
        "        if sigma_0 == 0:\n",
        "          sigma_0 = 1e-12\n",
        "        L[k] = -(k)/2*math.log(sigma_1**2) - (len(traj[n])-k)/2*math.log(sigma_2**2) + (len(traj[n]))/2*math.log(sigma_0**2)\n",
        "        cri_val[k] = CriVal(k, 1-0.15)\n",
        "      ## end for\n",
        "\n",
        "      #find the point R greater than the critical where it is most likely for a change point to have occured\n",
        "      Z = np.sqrt(2*L)\n",
        "      z_greater = np.greater(Z, cri_val)\n",
        "      Z = np.multiply(Z, z_greater) \n",
        "      R = np.argmax(Z)\n",
        "      \n",
        "      #split data around change point R and adjust trajectory array accordingly\n",
        "      f = np_split(traj[n],R)\n",
        "      if len(f[0]) > 0:\n",
        "        traj[n] = f[0]\n",
        "        traj = np_insert(traj, n+1, f[1])\n",
        "    ## end while 2\n",
        "    n = n+1\n",
        "  ## end while 1\n",
        "\n",
        "  #convert delta_points into the real_point locations of the change points\n",
        "  prev_cp = 0;\n",
        "  all_cp = [0]\n",
        "  for n in range(len(traj)):\n",
        "    all_cp.append(len(traj[n])+prev_cp)\n",
        "    prev_cp = prev_cp + len(traj[n])\n",
        "\n",
        "  all_cp = np.unique(np.array(all_cp))\n",
        "\n",
        "  return all_cp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9YV6pZtQDlZo"
      },
      "outputs": [],
      "source": [
        "#@title NJIT AH ClusterN\n",
        "#This section does agglomerate hierarcy clustering. Essentially what is does is\n",
        "#   using the changepoints calculated previously determine the median intensity and the std_dev of each section\n",
        "#   and then do a log-likelyhood test to see which two sections should be combined until the data is all assigned to one intensity level\n",
        "#   all n of these groupings are returned\n",
        "\n",
        "#NJIT function to allow array concatination\n",
        "@njit\n",
        "def NJIT_concat(tr_1, tr_2):\n",
        "  tr_concat = np.zeros((len(tr_1)+len(tr_2)))\n",
        "  tr_concat[0:len(tr_1)] = tr_1\n",
        "  tr_concat[len(tr_1):len(tr_1)+len(tr_2)] = tr_2\n",
        "  return tr_concat\n",
        "\n",
        "# calculate the log-likelyhood of two segments belonging to the same state\n",
        "@njit\n",
        "def NJIT_MN_Calc(section_1_tr, section_1_intensity, section_1_std, section_2_tr, section_2_intensity, section_2_std):\n",
        "  temp = NJIT_concat(section_1_tr, section_2_tr)\n",
        "  comb_sigma = np.nanstd(temp)\n",
        "  if comb_sigma == 0:\n",
        "    comb_sigma = 1e-12\n",
        "  comb_I = np.median(temp)\n",
        "  return math.log(section_1_std*section_2_std/(comb_sigma**2)) - ((section_1_intensity - comb_I)**2 + (section_2_intensity - comb_I)**2)/(2*(comb_sigma**2))\n",
        "\n",
        "\n",
        "#delete array row/column in NJIT\n",
        "@njit\n",
        "def delete_workaround(arr, num):\n",
        "  new_arr = np.zeros((arr.shape[0]-1, arr.shape[1]-1))\n",
        "  for r in range(arr.shape[0]):\n",
        "    if r > num:\n",
        "      r_index = r-1\n",
        "    else:\n",
        "      r_index = r\n",
        "    for c in range(arr.shape[1]):\n",
        "      if c > num:\n",
        "        c_index = c-1\n",
        "      else:\n",
        "        c_index = c\n",
        "      if r != num and c != num:\n",
        "        new_arr[r_index][c_index] = arr[r][c] \n",
        "  return new_arr\n",
        "\n",
        "#turn array of different sized lists into a square matrix with empty cells filled by NaN\n",
        "@njit\n",
        "def NJIT_boolean_indexing(v):\n",
        "    lens = np.reshape(np.array([len(item) for item in v]), (-1,1))\n",
        "    mask = lens > np.arange(lens.max()+1)\n",
        "    mask = np.fliplr(mask)\n",
        "    out = np.zeros(mask.shape,dtype=float)\n",
        "    out[:] = np.NaN\n",
        "    for r in range(len(mask)):\n",
        "      item = 0\n",
        "      for c in range(len(mask[0])):\n",
        "        if mask[r,c]:\n",
        "          out[r,c] = v[r][item]\n",
        "          item += 1\n",
        "    return out\n",
        "\n",
        "#AH clustering process\n",
        "@njit\n",
        "def NJIT_AHclusterN(traj,cp):\n",
        "  cp.append(len(traj))\n",
        "  Ng_max = len(cp)\n",
        "\n",
        "  Yi_tr = [] \n",
        "  Yi_intensity = [0.0 for i in range(Ng_max)]\n",
        "  Yi_t = [0 for i in range(Ng_max)]\n",
        "  Yi_group = [np.array([0 for x in range(Ng_max)]) for i in range(Ng_max)]\n",
        "  Yi_std = [0.0 for i in range(Ng_max)]\n",
        "  Yi_comb_group = [np.array([0 for x in range(Ng_max)]) for i in range(Ng_max)]\n",
        "\n",
        "  # determine inital len(cp) sections intensities and std_devs\n",
        "  for i in range(Ng_max):\n",
        "    if i == 0:\n",
        "      start_ = 0\n",
        "      end_ = cp[i]+1\n",
        "    else:\n",
        "      start_ = cp[i-1]+1\n",
        "      end_ = cp[i]+1\n",
        "    Yi_tr.append(traj[start_:end_])\n",
        "    Yi_intensity[i] = np.median(Yi_tr[i])\n",
        "    Yi_std[i] = np.nanstd(Yi_tr[i])\n",
        "    if Yi_std[i] == 0:\n",
        "      Yi_std[i] = 1E-50\n",
        "    Yi_group[i][-1] = i\n",
        "    Yi_comb_group[i][-1] = i\n",
        "    Yi_t[i] = len(Yi_tr[i])\n",
        "\n",
        "  Yi_comb_tr = Yi_tr.copy()\n",
        "  Yi_comb_intensity = Yi_intensity.copy()\n",
        "  Yi_comb_t = Yi_t.copy()\n",
        "  Yi_comb_std = Yi_std.copy()\n",
        "\n",
        "  M_mn = [[NJIT_MN_Calc(Yi_comb_tr[m], Yi_comb_intensity[m], Yi_comb_std[m], Yi_comb_tr[n], Yi_comb_intensity[n], Yi_comb_std[n]) for m in range(n+1, len(Yi_comb_t))] for n in range(len(Yi_comb_t))]\n",
        "  M_mn = NJIT_boolean_indexing(M_mn).T\n",
        "\n",
        "  # combine states to have len(cp)-1:1 states\n",
        "  for i in range(len(Yi_t) - 1, 0, -1):\n",
        "    M_mn = np.ascontiguousarray(M_mn)\n",
        "    loc_mn = np.argwhere(M_mn==np.nanmax(M_mn))\n",
        "    loc_mn = loc_mn[0]\n",
        "    mn_max = np.amax(loc_mn)\n",
        "    mn_min = np.amin(loc_mn)\n",
        "\n",
        "    group_min = np.amin(np.array([Yi_comb_group[mn_max][i],Yi_comb_group[mn_min][i]]))\n",
        "    group_max = np.amax(np.array([Yi_comb_group[mn_max][i],Yi_comb_group[mn_min][i]]))\n",
        "\n",
        "    # change state assignments for the selected states to combine\n",
        "    for nn in range(len(Yi_group)):\n",
        "      if Yi_group[nn][i] == group_min or Yi_group[nn][i] == group_max:\n",
        "        Yi_group[nn][i-1] = group_min\n",
        "      else:\n",
        "        Yi_group[nn][i-1] = Yi_group[nn][i]\n",
        "\n",
        "    for nn in range(len(Yi_comb_t)):\n",
        "      Yi_comb_group[nn][i-1] = Yi_comb_group[nn][i]\n",
        "\n",
        "    # combine the two states\n",
        "    Yi_comb_tr[mn_min] = np.concatenate((Yi_comb_tr[mn_max], Yi_comb_tr[mn_min]))\n",
        "    Yi_comb_intensity[mn_min] = np.median(Yi_comb_tr[mn_min])\n",
        "    Yi_comb_std[mn_min] = np.nanstd(Yi_comb_tr[mn_min])\n",
        "    if Yi_comb_std[mn_min] == 0:\n",
        "      Yi_comb_std[mn_min] = 1E-50\n",
        "    Yi_comb_group[mn_min][i-1] = group_min\n",
        "    \n",
        "    Yi_comb_tr.pop(mn_max)\n",
        "    Yi_comb_intensity.pop(mn_max)\n",
        "    Yi_comb_t.pop(mn_max)\n",
        "    Yi_comb_group.pop(mn_max)\n",
        "    Yi_comb_std.pop(mn_max)\n",
        "\n",
        "    #delete set of rows/columns associated with segment that got comibined\n",
        "    M_mn = delete_workaround(M_mn, mn_max)\n",
        "\n",
        "    #recalculate values in M_mn affected by the combining of the two segments\n",
        "    M_mn[mn_min][:] = [NJIT_MN_Calc(Yi_comb_tr[mn_min], Yi_comb_intensity[mn_min], Yi_comb_std[mn_min], Yi_comb_tr[n], Yi_comb_intensity[n], Yi_comb_std[n]) \n",
        "    if mn_min > n else math.nan for n in range(len(M_mn[0]))]\n",
        "\n",
        "    M_mn[:][mn_min] = [NJIT_MN_Calc(Yi_comb_tr[m], Yi_comb_intensity[m], Yi_comb_std[m], Yi_comb_tr[mn_min], Yi_comb_intensity[mn_min], Yi_comb_std[mn_min]) \n",
        "    if mn_min > m else math.nan for m in range(len(M_mn[0]))]\n",
        "  \n",
        "  return Yi_tr, Yi_intensity, Yi_t, Yi_group, Yi_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rdooh1eU1F1b",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title NJIT EMclusterN\n",
        "# This takes the data from AHclusterN and decides which intensity levels are actually the same\n",
        "#   it runs on every returned grouping from AHclusterN and can only decrease the number of intensity levels\n",
        "\n",
        "#function allowing us to do array math in NJIT\n",
        "@njit\n",
        "def np_apply_along_axis(func1d, axis, arr):\n",
        "  assert arr.ndim == 2\n",
        "  assert axis in [0, 1]\n",
        "  if axis == 0:\n",
        "    result = np.empty(arr.shape[1])\n",
        "    for i in range(len(result)):\n",
        "      result[i] = func1d(arr[:, i])\n",
        "  else:\n",
        "    result = np.empty(arr.shape[0])\n",
        "    for i in range(len(result)):\n",
        "      result[i] = func1d(arr[i, :])\n",
        "  return result\n",
        "\n",
        "#function allowing us to do nansum in NJIT\n",
        "@njit\n",
        "def np_nansum(array, axis):\n",
        "  return np_apply_along_axis(np.nansum, axis, array)\n",
        "\n",
        "#function to take an array list and turn it into a 2d array\n",
        "@njit\n",
        "def make_2d(arraylist):\n",
        "  n = len(arraylist)\n",
        "  k = arraylist[0].shape[0]\n",
        "  a2d = np.zeros((n, k))\n",
        "  for i in range(n):\n",
        "      a2d[i] = arraylist[i]\n",
        "  return(a2d)\n",
        "\n",
        "\n",
        "@njit(parallel=False)\n",
        "def njit_EMclusterN(Yi_intensity, Yi_t, Yi_group, Yi_std):\n",
        "\n",
        "  #define the arrays of our different parameters that will be populated later\n",
        "  Yem_state = [[0 for i in range(len(Yi_t))] for j in range(len(Yi_t))]\n",
        "  Yem_intensity = [[0.0 for i in range(len(Yi_t))] for j in range(len(Yi_t))]\n",
        "  Yem_sigma = [[0.0 for i in range(len(Yi_t))] for j in range(len(Yi_t))]\n",
        "  Yem_prob = [[0.0 for i in range(len(Yi_t))] for j in range(len(Yi_t))]\n",
        "  Yem_pk = [[0.0 for i in range(len(Yi_t))] for j in range(len(Yi_t))]\n",
        "  Yem_nos = [[0 for i in range(len(Yi_t))] for j in range(len(Yi_t))]\n",
        "  \n",
        "  array_of_states = np.zeros((len(Yi_t), len(Yi_t)))\n",
        "  array_of_intensities = np.zeros((len(Yi_t), len(Yi_t)))\n",
        "  array_of_lengths = np.zeros((len(Yi_t), len(Yi_t)))\n",
        "  array_of_stddevs_sq = np.zeros((len(Yi_t), len(Yi_t)))\n",
        "\n",
        "\n",
        "  # create stock arrays of the various intensities, lengths and std_devs for all the segments\n",
        "  for r in range(len(Yi_t)):\n",
        "    for j in range(len(Yi_t)):\n",
        "      array_of_states[j, r] = Yi_group[r][j]\n",
        "      array_of_intensities[j,r] = Yi_intensity[r]\n",
        "      array_of_lengths[j,r] = Yi_t[r]\n",
        "      array_of_stddevs_sq[j,r] = Yi_std[r]**2\n",
        "\n",
        "\n",
        "  total_len = len(Yi_t)\n",
        "\n",
        "  for total_clusters in range(total_len):\n",
        "\n",
        "    #for runtime purposes complete maximization is only run on the AH results from 6 possible states on down\n",
        "    if total_clusters < 6:\n",
        "\n",
        "      #STEP 1\n",
        "      #p_mj is the array which represents the probability that state m belongs to state j\n",
        "      #it's initialized with the AH results\n",
        "\n",
        "      num_states = np.unique(np.transpose(array_of_states[total_clusters][:]))\n",
        "      j_length = len(Yi_group[0])\n",
        "      m_length = int(np.amax(num_states)+1)\n",
        "      delta_p_mj = 1\n",
        "\n",
        "      intensity_arr = array_of_intensities[:][0:m_length]\n",
        "      len_arr = array_of_lengths[:][0:m_length]\n",
        "      stddev_arr = array_of_stddevs_sq[:][0:m_length]\n",
        "\n",
        "      p_mj = np.zeros((j_length, m_length))\n",
        "      for n in range(j_length):\n",
        "        p_mj[n][Yi_group[n][total_clusters]] = 1\n",
        "      p_mj = np.transpose(p_mj)\n",
        "\n",
        "      iter = 0\n",
        "      #this while loop runs until there is a sufficently small change in p_mj or the maximum number of iterations is exceeded\n",
        "      while delta_p_mj > 1e-8 and iter < 1e4:\n",
        "\n",
        "        #STEP 2\n",
        "        old_p_mj = np.copy(p_mj)\n",
        "\n",
        "        #calculate the intensity (I_m), std_dev (sigma_m) and probabilties (P_m) of every state\n",
        "        p_mj_x_N_j = np.multiply(p_mj, len_arr)\n",
        "        I_m = np_nansum(np.multiply(intensity_arr, p_mj_x_N_j), axis = 1)/np_nansum(p_mj_x_N_j, axis = 1)\n",
        "        sigma_m = np.sqrt(np_nansum(np.multiply(stddev_arr, p_mj_x_N_j), axis = 1)/np_nansum(p_mj_x_N_j, axis = 1))\n",
        "        P_m = np_nansum(p_mj_x_N_j, axis = 1)/np_nansum(len_arr, axis = 1)\n",
        "\n",
        "        #STEP 3\n",
        "        #caculate the new p_mj from I_m, sigma_m and P_m\n",
        "        p_mj = make_2d([np.multiply(P_m, gauss(intensity_arr[0][j], I_m,sigma_m)) for j in range(j_length)])\n",
        "        bottom_sum = np.reshape(np.repeat(np.reshape(np_nansum(p_mj, axis = 1), (1,-1))[0],total_clusters+1), (-1,total_clusters+1))\n",
        "        p_mj = np.transpose(np.divide(p_mj,bottom_sum))\n",
        "\n",
        "        #calculate the difference\n",
        "        delta_p_mj = np.nansum(np.abs(np.subtract(p_mj, old_p_mj)))\n",
        "        iter = iter +1\n",
        "      ###end while\n",
        "\n",
        "      # truncate I_m and std_m to check if the assigned states are the same to the 1's place\n",
        "      I_m_trunc = np.array([int(a) for a in I_m if a!=0 and not math.isnan(a)])\n",
        "      std_trunc = np.array([int(a * 10**2)/10**2 for a in sigma_m if a!=0 and not math.isnan(a)])\n",
        "\n",
        "      I_m_lst = []\n",
        "\n",
        "      #assign the parameters of each state based on the condensed values and the maximum occupation probability in p_mj\n",
        "      for section in range(j_length):\n",
        "        if total_clusters != 0:\n",
        "          sec = p_mj[:,section][~np.isnan(p_mj[:,section])]\n",
        "          arg_max = np.argmax(sec)\n",
        "          sec2 = p_mj[:,arg_max][~np.isnan(p_mj[:,arg_max])]\n",
        "          Yem_prob[total_clusters][section] = np.sum(sec2)\n",
        "          Yem_nos[total_clusters][section] = len(sec)\n",
        "        else:\n",
        "          arg_max = 0\n",
        "          Yem_prob[total_clusters][section] = 1 \n",
        "          Yem_nos[total_clusters][section] = 1\n",
        "          \n",
        "        I_m_lst.append(int(I_m[arg_max]))\n",
        "        Yem_state[total_clusters][section]= arg_max\n",
        "        Yem_intensity[total_clusters][section] = I_m[arg_max]\n",
        "        Yem_sigma[total_clusters][section] = sigma_m[arg_max]\n",
        "        Yem_pk[total_clusters][section] = P_m[arg_max]\n",
        "    else:\n",
        "      # assign the AH data as the solution for these data\n",
        "      for section in range(j_length):\n",
        "        Yem_state[total_clusters][section] = Yi_group[total_clusters][section]\n",
        "        Yem_intensity[total_clusters][section] = Yi_intensity[total_clusters]\n",
        "        Yem_sigma[total_clusters][section] = Yi_std[total_clusters]\n",
        "        Yem_prob[total_clusters][section] = 1e-12\n",
        "        Yem_pk[total_clusters][section] = 1e-12\n",
        "        Yem_nos[total_clusters][section] = total_clusters\n",
        "\n",
        "  return Yem_state, Yem_intensity, Yem_sigma, Yem_prob, Yem_pk, Yem_nos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AY7dKkjnxBEf"
      },
      "outputs": [],
      "source": [
        "#@title get_state\n",
        "\n",
        "def get_state(Ns, k, Yem_intensity,Yem_sigma, Yem_pk):\n",
        "\n",
        "  # Extract parameters of states.\n",
        "\n",
        "  # state_para: \n",
        "  # 1st column: intensity levels of states.\n",
        "  # 2nd column: noise levels(standard deviation) of states.\n",
        "  # 3rd column: populations of states.\n",
        "  # the number of rows in state_para corresponds to the number of states,\n",
        "  # each row include the parameters from one state.\n",
        "  # other parameters are added later\n",
        "  # 4th column: <t> for each state\n",
        "  # 5th column: time of first dark event\n",
        "  \n",
        "  state_para = np.zeros([Ns,6])\n",
        "  g = 0\n",
        "  s = 0\n",
        "  while s < Ns:\n",
        "      if ~np.isin(Yem_intensity[k][g],state_para[:,0]):\n",
        "          state_para[s,0] = Yem_intensity[k][g]\n",
        "          state_para[s,1] = Yem_sigma[k][g]\n",
        "          state_para[s,2] = Yem_pk[k][g]\n",
        "          s = s + 1\n",
        "      g = g + 1\n",
        "      if g >= len(Yem_intensity):\n",
        "          break\n",
        "  \n",
        "  return state_para"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Whs9HV5cRPf8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title plotting_prework\n",
        "# this guy just gets some stuff ready for the plotting we want to do later\n",
        "\n",
        "def between(num, lower, upper):\n",
        "  return (num >= lower and num <= upper)\n",
        "\n",
        "\n",
        "def plotting_prework(k, state_para,Yi_t, Yem_state, data):\n",
        "  #sorts the returned state_para by its order of occurence in the assignment\n",
        "  temp = [Yem_state[k][n] for n in range(len(Yem_state))]\n",
        "  aaa, bbb = np.unique(temp, return_index=True)\n",
        "  aaa = aaa[np.argsort(bbb)]\n",
        "\n",
        "  state_para1 = np.zeros([int(np.amax(aaa)+1),6])\n",
        "  for n in range(len(aaa)):\n",
        "    state_para1[aaa[n],:] = state_para[n,:]\n",
        "\n",
        "  state_trace = np.empty([len(data),2])\n",
        "  state_trace[:] = 0\n",
        "\n",
        "  #defines the relevant ranges to account for some fast events\n",
        "  ranges = []\n",
        "  for n in range(len(state_para1)):\n",
        "    ranges.append([state_para1[n,0]-2.5*state_para1[n,1], state_para1[n,0]+2.5*state_para1[n,1]])\n",
        "\n",
        "\n",
        "  #assign each time-point to a state and intensity level\n",
        "  #also accounts for short time changes which changepoint doesn't pick up\n",
        "  end_ = 0\n",
        "  for n in range(len(Yi_t)):\n",
        "    start_ = end_\n",
        "    end_ = start_+Yi_t[n]\n",
        "    for x in range(start_, end_):\n",
        "      state_trace[x,0] = Yem_state[k][n]\n",
        "\n",
        "      if not between(data[x], ranges[int(state_trace[x,0])][0], ranges[int(state_trace[x,0])][1]):\n",
        "        state = 0\n",
        "        while state < len(state_para[:,0]) and not between(data[x], ranges[state][0], ranges[state][1]):\n",
        "          state = state+1\n",
        "        if state != len(state_para[:,0]):\n",
        "          state_trace[x,0] = state\n",
        "      state_trace[x,1] = state_para1[int(state_trace[x,0]), 0]\n",
        "\n",
        "  #figure out time of first dark event (defined as an event not in the highest intensity state) and return  \n",
        "  f = np.argmax(state_para1[:,0])\n",
        "  loc = np.argwhere(state_trace[:,0]!= f)\n",
        "\n",
        "  if len(loc) >= 1:\n",
        "    for n in range(len(state_para1)):\n",
        "      if len(state_para1[0,:]) >1:\n",
        "        state_para1[n,5] = loc[0]*0.2\n",
        "      else:\n",
        "        state_para1[n,5] = 999999\n",
        "  else:\n",
        "    state_para1[0,5] = 999999\n",
        "\n",
        "\n",
        "  return state_trace, state_para1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIZ0hrsXPydi",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title power law fitting\n",
        "#fits segmented data to power law (a*t^(-m)) or truncated power law(a*t^(-m)*e^(t/Tc))\n",
        "\n",
        "def calc_avg_t(b, t_min, t_max):\n",
        "  #returns average time for power law\n",
        "  return (b+1)/(b+2) * (t_max**(b+2) - t_min**(b+2))/(t_max**(b+1) - t_min**(b+1))\n",
        "\n",
        "def calc_avg_t_trunc(b, t_c, t_min, t_max):\n",
        "  #returns average time for truncated power law\n",
        "  f = -t_c*(mpmath.gammainc(b+2, -t_max/t_c) -mpmath.gammainc(b+2, -t_min/t_c))/(mpmath.gammainc(b+1, -t_max/t_c) -mpmath.gammainc(b+1, -t_min/t_c))\n",
        "  return float(mpmath.re(f))\n",
        "\n",
        "def power_law_fitting(state_trace, time, level_of_interest):\n",
        "  t = time[1]\n",
        "  time_bin = time[1]\n",
        "  lst = []\n",
        "\n",
        "  #find all events associated with an intensity level and record their durration\n",
        "  for n in range(len(time)-1):\n",
        "    if state_trace[n,0] == level_of_interest:\n",
        "      if state_trace[n,0] == state_trace[n+1,0]:\n",
        "        t += time_bin\n",
        "      else:\n",
        "        lst.append(t)\n",
        "        t = time_bin\n",
        "  lst.append(t+time_bin)\n",
        "\n",
        "  #histogram the intenisity events\n",
        "  counts = np.histogram(lst, bins = time)\n",
        "  counts_x = [counts[1][x] for x in range(len(counts[0])) if counts[0][x] != 0]\n",
        "  counts_y = [counts[0][x] for x in range(len(counts[0])) if counts[0][x] != 0]\n",
        "  counts_x.append(time[-1])\n",
        "\n",
        "  #calcuate the probability of an event lasting a durration t\n",
        "  prob_y = []\n",
        "  for n in range(len(counts_y)):\n",
        "    if n == 0:\n",
        "      prob_y.append(counts_y[n]/((counts_x[n+1] - time_bin)/2))\n",
        "    else:\n",
        "      prob_y.append(counts_y[n]/((counts_x[n+1] - counts_x[n-1])/2))\n",
        "\n",
        "  counts_x.pop(-1)\n",
        "\n",
        "  if len(counts_x) > 3:\n",
        "    #fit to linear power law\n",
        "    power_fit, power_cov = curve_fit(linear_power_law, counts_x, np.log(prob_y))\n",
        "    power_y = power_fit[0]*counts_x**(-power_fit[1])\n",
        "\n",
        "    #fit to truncated power law\n",
        "    trunc_power_fit, trunc_power_cov = curve_fit(linear_trunc_power_law, counts_x, np.log(prob_y))\n",
        "    trunc_power_y = trunc_power_fit[0]*counts_x**(-trunc_power_fit[1])*np.exp(counts_x/trunc_power_fit[2])\n",
        "\n",
        "    #determine R^2 of both fits\n",
        "    R_sq_power = R_sq(linear_power_law(counts_x, power_fit[0], power_fit[1]), np.log(prob_y))\n",
        "    R_sq_trunc_power = R_sq(linear_trunc_power_law(counts_x, trunc_power_fit[0], trunc_power_fit[1], trunc_power_fit[2]), np.log(prob_y))\n",
        "\n",
        "    #return the parameters of the fit\n",
        "    if R_sq_power > R_sq_trunc_power:\n",
        "      y_fit= power_fit[0]*counts_x**(-power_fit[1])\n",
        "      params = [power_fit[1], math.nan, R_sq_power, calc_avg_t(-power_fit[1], counts_x[0], counts_x[-1])]\n",
        "    else:\n",
        "      y_fit = trunc_power_fit[0]*counts_x**(-trunc_power_fit[1])*np.exp(counts_x/trunc_power_fit[2])\n",
        "      params = [trunc_power_fit[1],trunc_power_fit[2],R_sq_trunc_power, calc_avg_t_trunc(-trunc_power_fit[1], trunc_power_fit[2], counts_x[0], counts_x[-1])]\n",
        "  else:\n",
        "    params = [math.nan, math.nan, math.nan, np.average(lst)]\n",
        "    y_fit = [0]\n",
        "\n",
        "  return counts_x, prob_y, y_fit, params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9Uvn1x310d_D"
      },
      "outputs": [],
      "source": [
        "#@title NJIT_BIC_calc\n",
        "#calculate the BIC for each fitting to determine the most likely fitting\n",
        "\n",
        "@njit\n",
        "def NJIT_BIC_calc(Yi_t, Yi_tr, Yem_nos, Yem_state, Yem_intensity, Yem_sigma, Yem_pk, Yem_prob):\n",
        "  BIC = np.zeros((len(Yi_t),1))\n",
        "  total_points = 0\n",
        "  num_cp_2_lev = 0\n",
        "  cp_penalty_difference_2_lev = 0.0\n",
        "  double_sum_difference_2_lev = 0.0\n",
        "  \n",
        "  for m in range(len(Yi_tr)):\n",
        "    #for run time purpose we crop to a maximum to 6 possible states\n",
        "    if m <=6:\n",
        "      double_sum = 0\n",
        "      L_em = 0\n",
        "      G = Yem_nos[m][0]   \n",
        "      total_points = 0\n",
        "      num_segs = 1\n",
        "      last_seg = Yem_state[m][0] \n",
        "\n",
        "      for j in range(len(Yi_t)):\n",
        "        for i in range(Yi_t[j]): \n",
        "            if gauss(Yi_tr[j][i], Yem_intensity[m][j], Yem_sigma[m][j]) == 0:\n",
        "              #log(0) -> goes to -inf so we add a very large negative number\n",
        "              double_sum = double_sum + math.log(Yem_pk[m][j]) + -9999999\n",
        "            else:\n",
        "              double_sum = double_sum + math.log(Yem_pk[m][j]) + math.log(gauss(Yi_tr[j][i], Yem_intensity[m][j], Yem_sigma[m][j]))\n",
        "\n",
        "        if math.isnan(Yem_prob[m][j]):\n",
        "          L_em = L_em + np.log(1e-99)\n",
        "        else:\n",
        "          L_em = L_em + np.log(Yem_prob[m][j])\n",
        "\n",
        "        total_points = total_points + Yi_t[j]\n",
        "\n",
        "        if Yem_state[m][j] != last_seg:\n",
        "          num_segs += 1\n",
        "          last_seg = Yem_state[m][j]\n",
        "\n",
        "      BIC[m] = double_sum + L_em - (3/2)*G*math.log(num_segs) - num_segs*math.log(total_points)/2\n",
        "\n",
        "      if m == 0:\n",
        "        cp_penalty_difference_2_lev = num_segs*math.log(total_points)/2\n",
        "        double_sum_difference_2_lev = double_sum\n",
        "      if m == 1:\n",
        "        cp_penalty_difference_2_lev = abs(cp_penalty_difference_2_lev - (num_segs*math.log(total_points)/2))\n",
        "        double_sum_difference_2_lev = abs(double_sum_difference_2_lev- double_sum)\n",
        "        num_cp_2_lev = num_segs\n",
        "\n",
        "    else:\n",
        "      BIC[m] = -1E22  \n",
        "\n",
        "  return BIC, num_cp_2_lev, cp_penalty_difference_2_lev, double_sum_difference_2_lev "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-c6SE6veaj5w"
      },
      "outputs": [],
      "source": [
        "#@title Plotting\n",
        "#all the plotting to vizualize the calculations done here\n",
        "\n",
        "def Plotting(BIC, k,  Ns_range, state_trace, data, state_para, time, draw_plots):\n",
        "\n",
        "  bins_ = np.arange(np.amin(data)-10, np.amax(data)+10, 0.5)\n",
        "  h = np.histogram(data, bins=bins_)\n",
        "\n",
        "  #BIC PLOT\n",
        "  if draw_plots:\n",
        "    plt.subplot(2,4,1)\n",
        "      #plot BIC\n",
        "    BIC_x = np.arange(1, len(BIC)+1, 1)\n",
        "    plt.plot(BIC_x, BIC, color = my_colors[0], linewidth = 3, zorder = 0)\n",
        "      #draw circle around selected max location\n",
        "    plt.scatter(k+1, BIC[k], edgecolors = my_colors[-1], facecolors = 'none', zorder = 1, s = 100, linewidths = 3)\n",
        "    plt.xlabel(\"number of states\")\n",
        "    plt.ylabel(\"BIC\")\n",
        "    plt.ylim(np.amax([BIC[k]-3000, np.amin(BIC[0:6])-500]), np.amax(BIC)+100)\n",
        "    plt.xlim([1,7])\n",
        "    tick_settings(minor = False)\n",
        "    plt.xticks(np.arange(1, 8, 1))\n",
        "\n",
        "    #POWER LAW PLOTS\n",
        "    if Ns_range[k] > 1:\n",
        "      for n in range(2,3+int(np.amax(state_trace[:,0]))):\n",
        "          #do power law fitting\n",
        "        counts_x, prob_y, y_fit, params = power_law_fitting(state_trace, time, n-2)\n",
        "        state_para[n-2,3] = params[0]    #alpha\n",
        "        state_para[n-2,4] = params[3]    #avg time\n",
        "        plt.subplot(2,4,n)\n",
        "          # plot fit power law\n",
        "        if len(y_fit) > 1:\n",
        "          plt.loglog(counts_x, y_fit, color = my_colors[0], linewidth = 2)\n",
        "          # plot observed event lengths by probability\n",
        "        plt.scatter(counts_x, prob_y, color = my_colors[n-1])\n",
        "          # display parameters of the fitting\n",
        "        plt.text(np.amin(counts_x), np.amin(prob_y)+.005, \"state: \" + str(\"{:.0f}\".format(state_para[n-2,0]))+ \"\\n \\nalpha: \" + str(\"{:.2f}\".format(params[0])) +\"\\nT_c: \" + str(\"{:.2f}\".format(params[1])) + \"\\nR^2: \" + str(\"{:.2f}\".format(params[2])) + \"\\navg_t (s): \" + str(\"{:.2f}\".format(params[3])))\n",
        "        plt.xlabel('event time (s)')\n",
        "        plt.ylabel('event probability')\n",
        "        tick_settings(minor = True)\n",
        "\n",
        "\n",
        "    #STATE TRACE PLOT\n",
        "    plt.subplot(2,4,(5,7))\n",
        "      #plot data\n",
        "    plt.plot(time,data, color = my_colors[0], zorder = 0)\n",
        "    plt.xlim([-5,int(np.amax(time)+5)])\n",
        "    for n in range(0, int(np.amax(state_trace[:,0])+1)):\n",
        "      temp_trace = [state_trace[x,1]  if state_trace[x,0] == n else math.nan for x in range(len(state_trace[:,0]))]\n",
        "        #plot traces of each intensity state\n",
        "      plt.scatter(time, temp_trace, color = my_colors[n+1], linewidth = 3, s = 10, zorder = n+1, alpha=1)\n",
        "    plt.xlabel(\"time (s)\")\n",
        "    plt.ylabel(\"Intensity\")\n",
        "    tick_settings(minor = False)\n",
        "\n",
        "    #INTENSITY HISTOGRAM PLOT\n",
        "    plt.subplot(2,4,8)\n",
        "      #histogram the data\n",
        "    plt.hist(data, orientation='horizontal', bins= bins_, color = my_colors[0])\n",
        "    plt.ylabel(\"Intensity\")\n",
        "      #plot the CPA identified gaussian distributions\n",
        "    for n in range(len(state_para[:,0])):\n",
        "      bis_bis = bisect.bisect(bins_, state_para[n,0])-1\n",
        "      while h[0][bis_bis] == 0:\n",
        "        bis_bis += 5\n",
        "      height = h[0][bis_bis]\n",
        "      gaus = norm.pdf(bins_, state_para[n,0], state_para[n,1])\n",
        "        #print the gaussian parameters of the intensity states\n",
        "      plt.plot(gaus/np.amax(gaus)*height, bins_, linewidth = 5, color = my_colors[n+1])\n",
        "      t = plt.text(height+2,state_para[n,0], str(\"{:.0f}\".format(state_para[n,0])) + \" +/- \" + str(\"{:.0f}\".format(state_para[n,1])))\n",
        "      t.set_bbox(dict(facecolor='white', alpha=1, edgecolor='white'))\n",
        "    plt.xlabel(\"counts\")\n",
        "    tick_settings(minor = False)\n",
        "\n",
        "    plt.subplots_adjust(left=0.125, bottom=0.1, right=1.1, top=0.9, wspace=0.4, hspace=0.4)\n",
        "    fig = plt.gcf()\n",
        "    fig.set_size_inches(20, 10)\n",
        "  else:\n",
        "    if Ns_range[k] > 1:\n",
        "      for n in range(0,int(np.amax(state_trace[:,0]))+1):\n",
        "        #do power law fitting\n",
        "        counts_x, prob_y, y_fit, params = power_law_fitting(state_trace, time, n)\n",
        "        state_para[n,3] = params[0]    #alpha\n",
        "        state_para[n,4] = params[3]    #avg time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX0j4JcOwl4j",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title CP_MAIN\n",
        "# Determine the number of states and state parameters from input trajectory.\n",
        "# data: raw trajectory (in the format of row).\n",
        "# time: time points associated with each data point in the trajectory\n",
        "# draw_plots: T/F to indicate if the data should be plotted\n",
        "\n",
        "def CP_MAIN(data, time, draw_plots):\n",
        "  ###FIND THE CHANGEPOINTS\n",
        "  all_cp = findcp(data)\n",
        "  all_cp = list(all_cp)\n",
        "  if all_cp[-1] == len(data):\n",
        "    all_cp.pop(-1)\n",
        "\n",
        "  ###AH CLUSTERING\n",
        "  Yi_tr, Yi_intensity, Yi_t, Yi_group, Yi_std = NJIT_AHclusterN(data, all_cp)\n",
        "\n",
        "  Yi_intensity = np.array(Yi_intensity)\n",
        "  Yi_t = np.array(Yi_t)\n",
        "  Yi_group = np.array(Yi_group)\n",
        "  Yi_std = np.array(Yi_std)\n",
        "\n",
        "  #make sure that groups are in contiguous order\n",
        "  Yi_group_sort = Yi_group.T\n",
        "\n",
        "  for n in range(len(Yi_group_sort)):\n",
        "    group_lst = Yi_group_sort[n]\n",
        "    group_rank = rankdata(group_lst, method = 'dense')\n",
        "    Yi_group_sort[n] = group_rank-1\n",
        "\n",
        "  Yi_group = Yi_group_sort.T\n",
        "\n",
        "  ###EM OPTIMIZATION\n",
        "\n",
        "  Yem_state, Yem_intensity, Yem_sigma, Yem_prob, Yem_pk, Yem_nos = njit_EMclusterN(Yi_intensity, Yi_t, Yi_group, Yi_std)\n",
        "\n",
        "  Yem_state = np.array(Yem_state)\n",
        "  Yem_intensity = np.array(Yem_intensity)\n",
        "  Yem_sigma = np.array(Yem_sigma)\n",
        "  Yem_prob = np.array(Yem_prob)\n",
        "  Yem_pk = np.array(Yem_pk)\n",
        "  Yem_nos = np.array(Yem_nos)\n",
        "\n",
        "\n",
        "  #make sure that states are in contiguous order\n",
        "  for n in range(len(Yem_state)):\n",
        "    group_lst = Yem_state[n]\n",
        "    group_rank = rankdata(group_lst, method = 'dense')\n",
        "    Yem_state[n] = group_rank - 1\n",
        "\n",
        "  ###CALCULATE BIC\n",
        "  #Yi_tr - make square matrix\n",
        "  Yi_tr = NJIT_boolean_indexing(Yi_tr)\n",
        "  Yi_tr = np.fliplr(Yi_tr)\n",
        "\n",
        "  BIC, num_cp_2_lev, cp_penalty_difference_2_lev, double_sum_difference_2_lev = NJIT_BIC_calc(Yi_t, Yi_tr, Yem_nos, Yem_state, Yem_intensity, Yem_sigma, Yem_pk, Yem_prob)\n",
        "\n",
        "  #find ideal grouping\n",
        "  k = np.argmax(BIC)\n",
        "\n",
        "  ## the BIC penalty for the increased number of change points between a 1 and 2 level fit can outcompete the \n",
        "  ## goodness of fit factor in scenarios where the second level is mainly comprised of many short time events\n",
        "  ## these if statements check for this scenario\n",
        "  if k == 0:\n",
        "    #130 cp = 1.6% of the 8000 pt trace\n",
        "    if num_cp_2_lev > 0.016*len(data) and cp_penalty_difference_2_lev > double_sum_difference_2_lev:\n",
        "      k = 1\n",
        "\n",
        "  #get the parameters of the ideal grouping\n",
        "  Ns = Yem_nos[k][0]\n",
        "  Ns_range = [Yem_nos[n][0] for n in range(len(Yem_nos))]\n",
        "  state_para = get_state(Ns,k,Yem_intensity,Yem_sigma, Yem_pk)\n",
        "\n",
        "  state_trace, state_para = plotting_prework(k, state_para, Yi_t, Yem_state, data)\n",
        "\n",
        "  Plotting(BIC, k,  Ns_range, state_trace, data, state_para, time, draw_plots)\n",
        "\n",
        "  return state_para"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title initialize_and_compile\n",
        "# NJIT compilation significantly increases the runtime of the first trace passed - this is a dummy trace to get the compilation out of the way so\n",
        "#   NJIT can give us optimal runtime benifits\n",
        "def initialize_and_compile(int_time_s):\n",
        "  data = np.array([0,0,0,0,0,0,40,40,40,40,40,40,40,0,0,0,0,0,0,40,40,40,40,40,40,40])\n",
        "  time = [n*int_time_s for n in range(len(data))]\n",
        "  CP_MAIN(data, time, False)"
      ],
      "metadata": {
        "id": "T-Rf3lPX42Km",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code expects the following file tree:\n",
        "1. parent_folder = directory_path/Large Data Folder/analyzed_data/ (where large data folder is for example all videos collected on May 12th and analyzed_data is the folder created by the particle picking)\n",
        "2. qd_trace_folder = directory_path/Large Data Folder/analyzed_data/particle_picking/traces/ (save folder for the blinking traces extracted from all videos collected on May 12th)\n",
        "\n",
        "The code creates the following file tree:\n",
        "1. directory_path/Large Data Folder/analyzed_data/CPA_params/ (save folder for the data associated with CPA analysis on the blinking traces)\n",
        "2. directory_path/Large Data Folder/analyzed_data/CPA_params/Example CPA Fittings/ (save folder for the example CPA fit traces)"
      ],
      "metadata": {
        "id": "L0c5DH_FlDzP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1sCbbuKGFlX"
      },
      "outputs": [],
      "source": [
        "#change file paths to select your data\n",
        "parent_folder = \"//InsertYourPathHere/analyzed_data//\"   #change to master folder, remember to add the / for the end of path\n",
        "qd_trace_folder = parent_folder + \"particle_picking/traces/\"\n",
        "\n",
        "int_time_s = 0.05                 ## time bin width of the blinking trace in seconds\n",
        "\n",
        "start_trace = 1                   ## start at 1 for new trace - change to (n*auto_save)+1 for picking up in the middle of a batch\n",
        "\n",
        "auto_save = 50                    ## autosave CPA results every n traces\n",
        "\n",
        "\n",
        "onlyfiles = [f for f in listdir(qd_trace_folder) if isfile(join(qd_trace_folder, f))]\n",
        "\n",
        "para_folder = parent_folder + \"CPA_params/\"\n",
        "fig_folder = para_folder + \"Example CPA Fittings/\"\n",
        "\n",
        "if os.path.exists(para_folder) == False:\n",
        "  os.mkdir(para_folder)\n",
        "\n",
        "if os.path.exists(fig_folder) == False:\n",
        "  os.mkdir(fig_folder)\n",
        "\n",
        "\n",
        "#to speed up the runtime on the rather data heavy process that is CPA we use NJIT (a non-python compiler) since python is a particularly slow language\n",
        "#the first time NJIT functions are called they are much slower than the subsequent calls due to their compilation time\n",
        "#here we run a small fake blinking trace to compile all NJIT functions and benefit from the run time speed ups\n",
        "initialize_and_compile(int_time_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlDx60nhEWdh"
      },
      "outputs": [],
      "source": [
        "###### UNCOMMENT FOR SYNTHETIC DATA\n",
        "# data, time, act_cp = create_data(140, 100, 20, 0.05)          \n",
        "# para = CP_MAIN(data, time, True)\n",
        "######\n",
        "\n",
        "\n",
        "###### UNCOMMENT FOR REAL DATA\n",
        "for f in onlyfiles:\n",
        "  para_file_name = \"CPA Para \" + f\n",
        "  fig_file_name = f[:-4]\n",
        "  qd_traces = pd.read_csv(qd_trace_folder+ f)\n",
        "  qd_traces = qd_traces.to_numpy()\n",
        "  lst_para = []\n",
        "  time = [n*int_time_s for n in range(len(qd_traces[:,0]))]\n",
        "  \n",
        "  print(fig_file_name)\n",
        "\n",
        "  for n in range (start_trace, len(qd_traces[0])): #should start indexing at  one, slice zero is the bin numbers\n",
        "    try:\n",
        "      if n % 10 == 0:   #Fully plot every 10th trace and save the results for data quality checks\n",
        "        para = CP_MAIN(qd_traces[:,n], time, True)\n",
        "        plt.savefig(fig_folder+f'QD_{n}__'+fig_file_name+'.jpg', dpi = 300, bbox_inches = 'tight')\n",
        "        plt.close(plt.gcf())\n",
        "      else:\n",
        "        para = CP_MAIN(qd_traces[:,n], time, False)   #False - no plotting individual traces\n",
        "\n",
        "      #reshape the returned data and format it for our saving output\n",
        "      for j in range(len(para)):    \n",
        "        for k in para[j,:]:\n",
        "          lst_para.append(k)\n",
        "      for f in range(6):\n",
        "        lst_para.append(math.nan)\n",
        "\n",
        "      print(f\"{n} -- ({len(para)})\")    #trace number n completed and fit to (x) states\n",
        "    except:\n",
        "      print(f\"failed {n}\")              #an error was thrown and the trace didn't complete\n",
        "\n",
        "\n",
        "    #save the parameters when we hit a multiple of the autosave value\n",
        "    if n%auto_save == 0:\n",
        "      lst_para = np.array(lst_para).reshape([int(len(lst_para)/6), 6])\n",
        "      p_name = para_file_name[:-4] + f'({n}).csv'\n",
        "      pd.DataFrame(np.array(lst_para)).to_csv(os.path.join(para_folder,p_name))\n",
        "      lst_para = []\n",
        "\n",
        "  #save the traces from the last autosave to the end of the file\n",
        "  lst_para = np.array(lst_para).reshape([int(len(lst_para)/6), 6])\n",
        "  p_name = para_file_name[:-4] + f'({n}).csv'\n",
        "  pd.DataFrame(np.array(lst_para)).to_csv(os.path.join(para_folder,p_name))\n",
        "######"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}